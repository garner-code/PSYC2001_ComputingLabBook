[["index.html", "Data Analysis Skills for Psychology in R Computing Labs Information 0.1 The purpose of this coursebook 0.2 Structure of the Coursebook 0.3 Bugs are important 0.4 Big unknowns and little unknowns 0.5 Setting up R and RStudio", " Data Analysis Skills for Psychology in R Kelly G. Garner, Bart Cooley, Marios Panayi, Peter Lovibond 2026-02-06 Computing Labs Information 0.1 The purpose of this coursebook Welcome to the computing tutorials for PSYC2001. This coursebook has been designed to support your learning of statistics through hands-on practice with R. It accompanies the lab tutorial folders available on Moodle, and will guide you step-by-step through the data analysis components of the course. The focus is not just on memorising, but on learning how to think about data, ask good questions, and use R as a tool to answer them. 0.2 Structure of the Coursebook Each chapter introduces a key concept in statistics. You will see worked examples with code that runs in R. These examples show you how to apply data analysis and statistical techniques, step by step. Throughout the chapters, you will find exercises to help you complete code in your own scripts. Completing the code in your own scripts is important, because you will need these coding skills to complete your assignment for the course, and beyond! Throughout this course book there will be questions that encourage you to think about about or apply what you’ve learned in new situation. 0.3 Bugs are important Learning to code involves making lots of errors. When a piece of code doesn’t work, it’s because you have a ‘bug’ in your code. This can feel disheartening at first, because we often think that bugs are a sign of failure. But bugs are actually an important part of learning to code. They help you to understand how the code works, and they help you to learn how to debug your code. So, don’t be afraid of bugs! Embrace them as part of the learning process. In fact, once you’ve been coding long enough, you’ll become deeply suspicious when you don’t find bugs. Figure 0.1: Remember, we like bugs! They help us solve the issues with our code (Image courtesy of Copilot) 0.4 Big unknowns and little unknowns When learning about statistics and data analysis, you are often encountering lots of information about multiple different things for the very first time, such as p-values, statistical tests, and how software works. Sometimes, when learning to code a new analysis, it can be hard to know if your code isn’t working because you have missed something large and conceptual (a big unknown), or whether you have a bug in your code, such as a typo (little unknown). This course book aims to explain when you should be thinking about big unknowns (research questions), so that the rest of the time you can focus on the little unknowns during your computing labs. Solving little unknowns (fixing bugs) often requires reading your code very carefully, to check it matches the examples provided. You should also test what each little piece of code does, by checking what it takes as an input, and how it transforms the input into an output. How to do this will become clearer throughout the course, as we show you plenty of examples of how to do this. 0.5 Setting up R and RStudio All of the UNSW School of Psychology computers have R and R Studio installed, so you can use R and RStudio on those computers during your computing labs. Both R and RStudio are freely available so you can also install them on your own machine. There is a useful guide to installing them both here that you can use. If you have specific technical issues getting R and RStudio installed on your own laptop, then check the course moodle on how to get help. "],["introduction-to-r.html", "Chapter 1 Introduction to R 1.1 R and R Studio 1.2 Getting started 1.3 Getting to know R Studio 1.4 Scripts 1.5 Functions and arguments 1.6 Base R and packages 1.7 Installing and loading packages 1.8 Objects 1.9 Datatypes 1.10 Looking after the environment 1.11 The power of scripts 1.12 You are Free!", " Chapter 1 Introduction to R To perform data analysis in Psychology, one needs some powerful software to help you get data into shape, and to apply all the fancy statistical tests that you will learn about in this course. In this course, we will be using the programming language R and the software R Studio to do this. 1.1 R and R Studio For this course, you need two different bits of software, R and RStudio. R is a programming language that you will write code in and R Studio is an Integrated Development Environment (IDE) which makes working with R easier. Think of it as knowing English and using a plain text editor like NotePad* to write a book versus using a word processor like Microsoft Word. You could do it, but it wouldn’t look as good and it would be much harder without things like spell-checking and formatting. If you haven’t used a plain text editor, open either NotePad (Windows) or TextEdit (Mac) and do a bit of typing. Now imagine writing an assignment in it. In a similar way, you can use R without R Studio but we wouldn’t recommend it. The key thing to remember is that although you will do all of your work using R Studio for this course, you are actually using two pieces of software which means that from time-to-time, both of them may have separate updates. Figure 1.1: This is what using R without RStudio looks like. Noone wants that. 1.2 Getting started 1.2.1 Activity - Setting up your working folder You should have downloaded the files you need for this week’s computing lab tutorial from the course Moodle. Unzip the folder you have downloaded, by right-clicking over the file and selecting ‘Extract Here’ if you are on a PC, or ‘Open with… Archive Utility’ if you are on a Mac. Then move the folder to somewhere sensible on the computer, e.g. from ‘Downloads’ to ‘Documents’. Ask your tutor if you are unsure about any of those steps. Figure 1.2: Download the chapter folder and right click on it to unzip the contents Once the folder is unzipped and moved, open it up to take a peek at the contents inside. You should see something like the following - Figure 1.3: Every lab folder will contain all the files you need for the session Each week you will get a folder that looks somewhat like this. Each week, your first job is to download the folder, unzip it and move it to your sensible location. Next, navigate to the folder in your file explorer, so you can see the contents. Double click on the file that is of type R Project (e.g. for this lab, it is called ‘1_Introduction_to_R’ and in the image above you can see it is of the Type ‘R Project’. This will open RStudio. Every lab will have an R Project file, and you should always open RStudio by double clicking on it. Rproj stands for R project. A project can be thought of as all the files that live in the same folder as the .Rproj file. Having a reference to where these files are (which is what the .Rproj file gives you) will come in very handy in the next tutorial, and beyond. Why should I open RStudio by clicking on the .Rproj file? Indeed, this may seem like a strange request. An .Rproj file automatically tells R where your files are located. This is called ‘setting the working directory’. What this means is that you won’t have to manually tell R where on the computer your needed files live (why will become clearer in the next tutorial). This also means you get to save a lot of typing in future computing lab tutorials. 1.3 Getting to know R Studio R Studio has a console that you can try out code in (appearing as the bottom left window in Figure 1.4. If you have opened a script, then there is a script editor (top left, more on scripts shortly below). There is also a window showing functions and objects you have created in the “Environment” tab (top right window in the figure), and a window that shows files, plots, packages, and help documentation (bottom right). Figure 1.4: The RStudio interface: ah, that’s better! 1.3.1 Activity - Using the console First let’s get used to the console where you can try out code. You can think of the console as a very fancy calculator. You can do some impressive calculations, but like a calculator, once you close it down, everything you did is lost. Click on the Console (anywhere), and type the following: 2 + 2 Then press enter. You should see that R has calculated the answer for you. You can try other calculations, for example (you should also type this one into the console, and get the answer): (5 + 4 + 3) / 3 Important! If you are ever unsure about what a piece of code is doing, you should try running it (or bits of it) in the console, to see what answers each bit of code gives you. This is a great way to learn what code does and to debug your own code when it doesn’t work. 1.4 Scripts The console is great for small calculations and testing code, but it isn’t very good for doing analyses that you want to keep. For this, we use scripts. A script is simply a text file that contains code. You know you have an R script when the filename ends in ‘.R’. You can write code in a script and then run it in the console. This means that you can save your code and come back to it later. It also means that anyone, anywhere else in the world can reproduce your analysis by running your script. This is a key part of open science and reproducible research. 1.4.1 Activity - Getting to know scripts Open the script your-first-script.R in R Studio. You can do this by clicking on the file in the Files tab (bottom right window) or by using File &gt; Open File… from the top menu. You can see at the top of the script there are some comments. Comments are lines of text that start with a # symbol. R ignores these lines when it runs the code, but they are useful for you to write notes to yourself about what the code is doing. You should always include comments in your code to explain what it is doing. This helps you remember what you did when you come back to it later, and it helps others understand your code if you share it with them. Figure 1.5: Scripts can run your code as many times as you please. Try typing 2 + 2 into the script, below the comment that says # Type 2 + 2 here. Now try running that bit of code in the script. You can do this by highlighting the line(s) of code you want to run and then clicking the ‘Run’ button at the top of the script window (see Figure 1: Run button). You should see that the answer appears in the console. Figure 1.6: Highlight your code and then run it. Important life hack! Another way to do this is to highlight the code you want to run and then press Ctrl + Enter (or Cmd + Enter on a Mac). Try this now. You should now see the result in the console. Figure 1.7: We all want results. Remember to save your script regularly by clicking the save icon (or using Ctrl + S or Cmd + S on a Mac). Save it, save it now! 1.5 Functions and arguments Functions allow you to perform tasks that would take a long time to write out by hand, by only using one word! Think of them as nifty tools that will save you lots of typing, again and again. A function normally takes a number of arguments. An argument isn’t a sign of discontent, but rather, is a way of giving a function the bits of information it needs to work. You can look up all the arguments that a function takes by using the help documentation. You get help by using the format ?function. Before, we calculated (5 + 4 + 3) / 3 in the console. You may have noticed that we add three numbers and then divide by the total number of numbers we have. You may have also noticed that this is the formula for calculating the mean (or average) of a set of numbers. Often in Psychology, we want to calculate the mean when we have many more than three numbers. This is where functions come in handy. Let’s look at the function mean(). 1.5.1 Activity - using a function Look up the help documentation for mean() by typing ?mean in the console. You will see that there is a Description of the function, a guide to it’s Usage, and the Arguments you need to make the function work. In the Usage section, we see that mean() takes the following form: ## Default S3 method: mean(x, trim = 0, na.rm = FALSE, ...) In the Arguments section, there are explanations for each of the arguments. x is the set of numbers that we want to calculate the mean for. Do not worry about trim and na.rm. For now, they can remain as mythical arguments in the interest of simplicity. Let’s use the mean function. Highlight the following line of code in your script and run it: # run the below line of code to see what answer you get mean(x=c(5, 4, 3)) The output in the console should match the answer you got when you calculated (5 + 4 + 3) / 3 by hand. This is but a simple example, but I am sure you can imagine the power of functions when you have many more numbers to work with. You’ll see in the code above that we have put the numbers together in some brackets preceded by the letter c c(). You can think of c as standing for concatenate. c() is actually a function, that combines the numbers into something called a vector. You can think of a vector as a list of items of the same type (e.g., all numbers, or all words). Its handy to keep things in vectors as it means you can work on all the things in the vector at the same time, such as adding 2 to every element using c(1,2,3) + 2. Try typing c(1,2,3) + 2 into the console (and hitting enter) to see what happens. Getting help on help It can be difficult to understand help documentation when you are first learning R. Know that you will not be the first. Many cries for help have been posted on stackoverflow.com. You can look there. Also, asking AI like ChatGPT can be useful. For example, you could ask “What does the mean() function in R do?” and it will give you a plain English explanation. However, be careful as AI can sometimes give incorrect information. Always double-check with the official help documentation or other reliable sources. And always always always triple check any code you get AI to help you write. 1.5.2 Argument names In the above example, we wrote out the argument name when using the mean (i.e., x), however, this is not strictly necessary. The following two lines of code would both produce the same result: mean(x=c(5, 4, 3)) mean(c(5, 4, 3)) Importantly, if you do not write out the argument names, R will use the default order of arguments, so, health warning, make sure you check the order when there is more than 1 argument! If you write out the argument names then you can write the arguments in whatever order you like: mean(trim=0, x=c(5, 4, 3), na.rm=FALSE) When you are first learning R, you may find it useful to write out the argument names as it can help you remember and understand what each part of the function is doing. However, as your skills progress you may find it quicker to omit the argument names and you will also see examples of code online that do not use argument names so it is important to be able to understand which argument each argument is referring to (or look up the help documentation to check). In this course, we will always write out the argument names the first time we use each function, however, in subsequent uses they may be omitted. 1.6 Base R and packages When you install R you will have access to a range of functions including options for data wrangling and statistical analysis. The functions that are included in the default installation are typically referred to as Base R and there is a useful cheat sheet that shows many Base R functions here. However, the power of R is that it is extendable and open source - put simply, if a function doesn’t exist or doesn’t work very well, anyone can create a new package that contains data and code to allow you to perform new tasks. You may find it useful to think of Base R as the default apps that come on your phone and packages as additional apps that you need to download separately. 1.7 Installing and loading packages Info: The UNSW psychology computers will already have all of the packages you need for this course so you only need to install packages if you are using your own machine. 1.7.1 Activity - Install the tidyverse In order to use a package, you must first install it. The following code installs the package tidyverse, a package we will use very frequently in this course. If you are interested in learning more about tidyverse and how incredibly useful it is in R, consider going through some of the chapters and exercises in R for Data Science. It’s also a great reference whenever you need help using functions from the tidyverse. If you want to learn more coding skills in R, we highly recommend working your way through this book. If you are working on your own computer, use the below code to install the tidyverse. You can either copy this command into the console, or into your script for highlighting and running. install.packages(&quot;tidyverse&quot;) You only need to install a package once, however, each time you start R you need to load the packages you want to use, in a similar way that you need to install an app on your phone once, but you need to open it every time you want to use it. To load packages we use the function library(). Typically you would start any analysis script by loading all of the packages you need, but we will come back to that in the later labs. 1.7.2 Activity - Load the tidyverse Run the below code in the console to load the tidyverse. You can do this regardless of whether you are using your own computer or a University machine. library(tidyverse) You will get what looks like an error message - it’s not. It’s just R telling you what it’s done. Now that we’ve loaded the tidyverse package we can use any of the functions it contains but remember, you need to run the library() function every time you start R. 1.8 Objects A large part of your coding for data analysis will involve creating and manipulating objects. Objects contain stuff. That stuff can be numbers, words, or the result of operations and analyses.You assign content to an object using &lt;-. 1.8.1 Activity - Create some objects Copy and paste the following code into the script editor, change the code so that it uses your own name and age and run it. You should see that name, age, today, new_year, and data appear in the environment pane. name &lt;- &quot;emily&quot; age &lt;- 15 + 18 today &lt;-Sys.Date() new_year &lt;- as.Date(&quot;2025-01-01&quot;) data &lt;- rnorm(n = 10, mean = 15, sd = 3) M_values &lt;- mean(c(5,4,3)) Figure 1.8: Objects in the environment Note that in these examples, name,age, and new_year would always contain the values emily, 33, and the date of New Year’s Day 2025, however, today will draw the date from the operating system. data has been created by randomly drawing a set of 10 values from a normal distribution with a mean of 15 and a standard deviation of 3. See ?rnorm for more information on how this function works. You should recognise by now what has been saved to M_values. Warning: You may also see objects referred to as ‘variables’. There is a difference between the two in programming terms, however, they are used synonymously very frequently. As a side note, if you ever have to teach programming and statistics, don’t use your age as an example because everytime you have to update your teaching materials you get a reminder of the fragility of existence and your advancing age. Importantly, objects can be involved in calculations and can interact with each other. For example, copy and paste this code into your script and run it. Check your answer matches what you see below. age + 10 ## [1] 43 Finally, and importantly, you can store the result of these operations in a new object. Now copy and paste this code into your script, and run it decade &lt;- age + 10 Tip: You may find it helpful to read &lt;- as an instruction to make an object, e.g., make an object called name and put the text emily in that object. 1.9 Datatypes You will constantly be creating objects throughout this course and you will learn more about them and how they behave as we go along. For now it is enough to understand that they are a way of saving values, that these values can be numbers, text, or the result of operations, and that they can be used in further operations to create new variables. 1.9.1 Activity - Explore datatypes For now, we can have a look at the datatypes of our objects using the function typeof. You will see the below lines of code in your script. Run them and make sure you get the same answers as you see here. typeof(age) ## [1] &quot;double&quot; typeof(new_year) ## [1] &quot;double&quot; typeof(name) ## [1] &quot;character&quot; Info: There are 5 main datatypes: double, integer, complex, logical and character. Integers are always whole numbers, whereas objects of type double can have decimals. We will learn about using these datatypes (as well as some of the others) throughout this course, so don’t fret if you don’t understand it yet! 1.10 Looking after the environment If you’ve been writing a lot of code you may find that the environment pane (or workspace) has become cluttered with many objects. This can make it difficult to figure out which object you need and therefore you run the risk of using the wrong data. If you’re working on a new dataset, or if you’ve tried lots of different code before getting the final version, it is good practice to remember to clear the environment to avoid using the wrong object. You can do this in several ways. To remove individual objects, you can type rm(object_name) in the console. Complete the code in your script to remove the object data that we created earlier. To clear all objects from the environment run rm(list = ls()) in the console. To clear all objects from the environment you can also click the broom icon in the environment pane. Do it, do it now! Figure 1.9: Clearing the workspace 1.11 The power of scripts Now that you have cleared your Environment, you can learn the true power of scripts. They allow you to recreate your entire analysis from scratch, just by running the script. 1.11.1 Final Activity - Run the script The last activity for the day…highlight all the code in your script and run it. You should see all of the objects you created earlier magically reappear in the environment pane. 1.12 You are Free! Congratulations you have survived the first coding lab for this course. We look forward to seeing you next week for more coding fun. Figure 1.10: Bring It Credit: This chapter heavily borrows from the work of Emily Nordmann at the University of Glasgow. Thanks, Emily! https://psyteachr.github.io/ads-v3/01-intro.html "],["data-wrangling-and-visualisation.html", "Chapter 2 Data wrangling and visualisation 2.1 Checking installation and loading packages 2.2 What do packages do? 2.3 What do these packages do? 2.4 Organisation and CSV files 2.5 Importing your Data in R 2.6 Having a look at our data 2.7 Checking the quality of our data 2.8 Cleaning the data 2.9 Introducing Piping 2.10 Exporting Data 2.11 Data visualization using ggplot() 2.12 You are Free! 2.13 ⭐ Bonus exercises", " Chapter 2 Data wrangling and visualisation Data analysis in Psychology and any other discipline is 90% data wrangling and 10% actual analysis. This week we will start to understand how to manipulate and visualise data using R. As usual, download the zipped lab folder from the Moodle page for this week and unzip it somewhere sensible on your computer. Double click the .Rproj file to open RStudio, and open the.R script file to get started. 2.1 Checking installation and loading packages Before we can begin any script we first need to make sure that the required packages are installed in our version of RStudio. Next, we can load the required packages to be used in the script, using the library() function. R Coders always start their scripts by loading all the packages need to run all the code contained within the script. 2.1.1 Activity - Load your packages Copy and paste the code block below into your script in RStudio and try running it. If you need a reminder on how to run a piece of code then check back to Section 1.4.1. # Check if packages are installed, if not install. if(!require(here)) install.packages(&#39;here&#39;) #checks if a package is installed and installs it if required. if(!require(tidyverse)) install.packages(&#39;tidyverse&#39;) library(here) #loads in the specified package library(tidyverse) Note that in the code above, we’ve added a fancy if() statement to check if the packages are installed. if() will check the statement inside its brackets. If that statement returns a value of TRUE, then the code that comes after the brackets will be executed (e.g. it will install the package here). Checking packages are installed in your script is good practice as it ensures that your code runs smoothly on any computer without needing to manually install packages. Particularly if you are using the computers in the School of Psychology (UNSW), where we have already installed the packages you need (and we ask you not to install any more!). 2.2 What do packages do? You should be able to see that we have installed and loaded 2 different packages. Let’s first go over the basics of what a package is. In its simplest terms, a package is a toolbox that someone has created for us in R that makes our life easier. These packages build on the basic code that comes with the R programming language (what RStudio uses to run), called base R. Figure 2.1: The equivalent of loading an R package 2.3 What do these packages do? It is always a good idea to check the documentation for a package before you use it. We can do this by using the help syntax, which is the ?. The package we are trying to get help with is called here. 2.3.1 Activity - find out about packages Copy and paste the below line of code to find out more about the here package. This will open a help page that tells us the purpose of the package and how it works. ?here #? loads the documentation for a specified package. Handy hint: If you find the help page too overwhelming, try scrolling to the bottom and looking for the ‘Examples’ section. This will show you some simple code that uses the package. 2.4 Organisation and CSV files By looking at the Files pane, you should notice that the folder you downloaded for this lab contains a few extra files and folders, compared to last lab. Generally, the file and folder structure will follow the same format for the remaining labs. Here is a quick guide to what is in the folder: An R Project file called ‘2_Data_wrangling_and_visualization.Rproj’ Use this to open RStudio. A script called ‘data_wrangling_and_visualization.R’. A text file called README.txt’, which contains information about the data. It is exceptionally good practice to make sure that your projects contain a README file that explains what the data are about. Hint: always read the README file. A sub-folder called Data, containing a .csv file called ‘PSYC2001_social-media-data.csv’ A sub-folder called Output, which is where all your output will go. Note that R sees both sub-folders (and their contents) as being in your working directory - aka as part of your R project. These are the key ingredients needed to organise all projects in R. Figure 2.2: Project Organisation 2.4.1 Activity - getting to know the data files Click on the link to the Data folder in the files pane You will see that the folder contains the data that we will use in this lab, and the data file is called PSYC2001_social-media-data.csv, is a csv file (short for a Comma Separated Value file). Remember to click the two dots .. next to the green up arrow, to get back to the top level of your project folder (that’s the one you started out in when you double clicked the .Rproj file and looked at the Files pane in RStudio). Now, click on the README.txt file to learn more about the data, if you haven’t already. And if you haven’t already, why not? Always read the README file. Clicking on README.txt will open the file in the script editor window, in a tab next to your script. csv files are a common and handy way to store data, because they can be read by heaps of different programs, like Excel, Google Sheets, and R. They are simple text files where each line represents a row of data, and the values in each row are separated by commas. Use your File Explorer (Windows) or Finder (Mac) to navigate to the folder you have unzipped for this lab (2_Data-wrangling-and-visualisation). Try double clicking on the .csv file now to see what happens. You’ll see that Excel offers to open it for you. Click ok to open the file in Excel and take a look at how the data looks. Then say ‘that’s very kind of you Excel, but we are more powerful than you. We R.’ Figure 2.3: Social Media Data in Excel Next, click again on the ‘Data’ folder in the Files pane in RStudio, and click on the link to the .csv file. You will see an option to view the file. Click on it and see what happens. Figure 2.4: Option to view the file contents within RStudio The data file opens in the viewer of RStudio. Like this. Here it is. csv data in the raw. Figure 2.5: .csv data, pure and unadulterated Now, make sure you click on the two dots (..) at the top, next to the green arrow, to get back to the top level folder of the project. 2.5 Importing your Data in R What we want to do now is to import our data into R, using code that understands our data fields are separated by commas. We are now going to load our first dataset into R. To do this we will need to import the dataset using a function capable of importing csv files. 2.5.1 Activity - Import the data We will be using two different functions to achieve this. The read.csv() function is used to import our csv dataset and it comes from the utils package which is part of base R. But the read.csv() function needs to know where the file lives on the computer. To do this, we use the here() function from the here package. This function tells R the location of the project we are working from, to make locating the data easier. Let’s first confirm that here() knows our current location on this PC (called the ‘Working Directory’), by typing the following code into the console and hitting enter: here() This should return the file path to the folder where the .Rproj file is located. This is the very file path where you unzipped and moved your folder to at the start of the tutorial. Because here can return where the folder lives (i.e. the file path), we can use this to easily find where our file is located and read it in. Thanks, here! You will see in your script that we’ve started this line of code for you. Copy and paste what is missing into your script and run it (or type it yourself, if you are seeking digital liberty). social_media &lt;- read.csv(file = here(&quot;Data&quot;,&quot;PSYC2001_social-media-data.csv&quot;)) #reads in csv files If you are still unsure what we mean by ‘file path’ then please google it, google it now. Now that we have read in our data, we have saved our data to an object called social_media. You can see this in the Environment tab in the top right section of your screen. Importantly, this particular object is a dataframe. A dataframe is a special type of object in R that is used to store data tables. It is similar to an Excel spreadsheet. Dataframes are very useful for data analysis because they allow us to easily manipulate and analyze data. Warning: If you have an error, that’s all part of coding! But do make sure you ask your tutor for help :) 2.6 Having a look at our data Our data should now be imported into R! Recall from the README.txt file (you definitely should have read this by now) that this dataset was collected as part of an experiment investigating social media use in young Australian adults. Sixty young adults answered questions about their social media usage as well as their political attitudes. Data about their social media usage (e.g., likes) was collected while they used their preferred platforms under various conditions. The variables in the data are: id – a unique identifier (S1–S60) age – age in years time_on_social – average hours/day on social media (self-report diary) urban – urban (1) or rural (2) area (based on postcode density) good_mood_likes – likes/10 min during a good mood (from platform + diary) bad_mood_likes – as above, but during bad mood followers – average number of followers across platforms The next 3 columns are political attitude subscales: informed – how politically informed they feel (e.g., read news daily) campaign – how much they engage in campaign-related discussion activism – involvement in activism (e.g., protests, petitions) We should now check that we have imported into Rstudio matches this description (and what we saw when we opened it in Excel). There are a couple of ways to do this. 2.6.1 Activity - View the data. Always, always, view the data. It is incredibly important to have a look at the data you are analysing. This will quickly tell you if what you are planning to do is sensible, or if things are likely to go horribly wrong. The main thing to remember when running analysis is: garbage in, garbage out. If your data are rubbish, your results will be rubbish. So always check your data first. The first way is to manually click through to the dataset. Click on Environment in the top right section of your screen. Click on social_media. You should see a new tab pop up with the data in a table-like format (this is called a dataframe). Make sure that this new tab looks similar to what you saw when you opened Excel file. Figure 2.6: Navigating to dataset. Note that the person who made this fancy gif had their R Studio in dark mode, so the colours are different. Both your screen, and your RStudio, are working perfectly fine. We can also do this programmatically using the code below. We’re taking you through some different ways of viewing data. Type the below into the console and run it. # Method 1 - Type in the name of the object social_media This will print the entire dataset to the console. This is not ideal for large datasets, but it works ok-ish for small ones like this. Now adjust the next bit of code in your script so that it exactly matches what you see below, and run it. You can also see from the comments how the data will appear. # Method 2 - Use the View function View(social_media) #view automatically displays the dataset in a tab. Sometimes, you only want to get a sense of what the data looks like, without printing the whole thing to the console or opening a new tab. The next two methods do just that. Now complete the next lines in your script so that they match the below lines of code, and run each line, one at a time. # Method 3 - Use the head function head(social_media) #head displays the first 6 rows of each variable. ## id age time_on_social urban good_mood_likes bad_mood_likes followers ## 1 S1 15.2 3.06 1 22.8 46.5 173.3 ## 2 S2 16.0 2.18 1 46.0 48.3 144.3 ## 3 S3 16.8 1.92 1 50.8 46.1 76.5 ## 4 S4 15.6 2.61 1 29.9 29.2 171.7 ## 5 S5 17.1 3.24 1 37.1 52.4 109.5 ## 6 S6 15.7 2.44 1 26.9 20.2 157.5 ## polit_informed polit_campaign polit_activism ## 1 2.3 3.2 3.6 ## 2 1.6 2.2 2.6 ## 3 1.9 2.7 3.0 ## 4 1.6 2.3 2.6 ## 5 2.0 2.9 3.3 ## 6 2.4 3.4 3.9 # Method 4 - Use the str function str(social_media) #displays an overall summary of the object and variable structure. ## &#39;data.frame&#39;: 60 obs. of 10 variables: ## $ id : chr &quot;S1&quot; &quot;S2&quot; &quot;S3&quot; &quot;S4&quot; ... ## $ age : num 15.2 16 16.8 15.6 17.1 15.7 19.7 18.6 19.6 15.5 ... ## $ time_on_social : num 3.06 2.18 1.92 2.61 3.24 2.44 1.46 1.52 1.92 2.1 ... ## $ urban : int 1 1 1 1 1 1 1 1 1 1 ... ## $ good_mood_likes: num 22.8 46 50.8 29.9 37.1 26.9 14.8 26 6.5 45.7 ... ## $ bad_mood_likes : num 46.5 48.3 46.1 29.2 52.4 20.2 35.1 35.8 12.2 32.8 ... ## $ followers : num 173.3 144.3 76.5 171.7 109.5 ... ## $ polit_informed : num 2.3 1.6 1.9 1.6 2 2.4 1.7 1.6 1.5 2.2 ... ## $ polit_campaign : num 3.2 2.2 2.7 2.3 2.9 3.4 2.4 2.2 2.1 3.1 ... ## $ polit_activism : num 3.6 2.6 3 2.6 3.3 3.9 2.7 2.6 2.4 3.5 ... You should now have a good idea of what PSYC2001_social-media.csv looks like in RStudio. You should also be able to see that its a lot like what we saw in excel. Check back to Section 1.9.1 if you need a reminder of what int refers to. You will also notice that the last function, str(), displays a summary of the object. This includes: The object type (a dataframe) The number of observations/rows (60) The number of variables/columns (10) The datatype: chr for id, and num for all other variables Question: Please discuss with your deskmate and tutor what you think chr and num mean. Figure 2.7: You thinking 2.7 Checking the quality of our data Once we have imported our dataset into R, it’s important to check the quality of the data. One simple way to do this is by using the summary() function. 2.7.1 Activity - Summarise the data Copy and paste the below line of code into your script and run it. summary(social_media) #summary provides a quick overview of the data in each variable. ## id age time_on_social urban good_mood_likes ## Length:60 Min. :13.90 Min. :-999.000 Min. :1.0 Min. : 6.50 ## Class :character 1st Qu.:15.70 1st Qu.: 1.920 1st Qu.:1.0 1st Qu.:31.60 ## Mode :character Median :16.50 Median : 2.365 Median :1.5 Median :45.90 ## Mean :16.87 Mean : -30.845 Mean :1.5 Mean :43.04 ## 3rd Qu.:17.43 3rd Qu.: 3.042 3rd Qu.:2.0 3rd Qu.:53.40 ## Max. :23.00 Max. : 4.320 Max. :2.0 Max. :89.20 ## bad_mood_likes followers polit_informed polit_campaign polit_activism ## Min. :12.20 Min. : 61.40 Min. :0.600 Min. :0.800 Min. :0.900 ## 1st Qu.:39.08 1st Qu.: 76.47 1st Qu.:1.500 1st Qu.:2.100 1st Qu.:2.400 ## Median :49.30 Median :116.30 Median :1.800 Median :2.550 Median :2.900 ## Mean :49.84 Mean :124.76 Mean :1.858 Mean :2.602 Mean :2.977 ## 3rd Qu.:58.75 3rd Qu.:153.75 3rd Qu.:2.200 3rd Qu.:3.100 3rd Qu.:3.500 ## Max. :91.20 Max. :336.50 Max. :3.400 Max. :4.800 Max. :5.500 Question: Do you notice anything unusual in the output of this data ? Discuss with your neighbour and tutor. Hint: Take a closer look at the time_on_social variable. 2.8 Cleaning the data It should now be clear that these data are unusual because it has a minimum value of -999 in the time_on_social variable which is measured in hours (we can’t have negative time!). Figure 2.8: Negative time would be back to the future! A good question to ask now is - why are these values in the dataset? Sometimes when collecting data, we can’t get a response from every participant. Instead of leaving a blank, researchers will sometimes put in a placeholder value like -999 to show that the data are missing. These aren’t real numbers; they just mean the data wasn’t recorded. But -999 isn’t the standard way to show missing data in R. R uses NA to represent missing values, and that’s important because most R functions know how to handle NA properly but they don’t know to ignore -999. Why NA you ask? NA stands for Not Available. Which is arguably a lot clearer in meaning than -999. 2.8.1 Activity - Find and replace -999 values Lets first have a look at how many -999 values are present in the data. We can do this by using the filter() function from the tidyverse package which is used to keep (or remove) rows based on certain conditions. filter() requires two arguments, the first is the data that you want to filter rows out of, and the second is the logical condition you want to use to tell R exactly which rows to filter down to (i.e. this is basically a true or false game where you tell R which rows in the data frame are the TRUE ones you want, so that filter() knows which rows to return). Here we want to tell R to give us the rows where time_on_social is equal to -999, so we use a very important operator, which is ==. A double equals sign means ‘is equal to’. So here we ask R to filter the social_media data frame down to only the rows where the value for time_on_social is equal to -999. Complete the line of code in your script so that it matches the below. Then run both lines to see what happens. social_media_filtered &lt;- filter(social_media, time_on_social == -999) #keep all rows where `time_on_social` is equal to -999 View(social_media_filtered) #view the filtered dataframe Handily, we can then use the count() function from the tidyverse package to sum the number of rows left over in the resulting data frame. This tells us for how many people the time_on_social data is not available. Run the following line of code to see what count() does. count(social_media_filtered) #count the number of rows in the filtered dataframe) ## n ## 1 2 2.9 Introducing Piping A short aside to introduce a very special operation called a ‘pipe’ or %&gt;%. This operation is part of the tidyverse package and allows you to pass the result from one function to the next seamlessly in a sort of assembly-line like fashion. You can think of it as saying “take the output from the last function and then do the next thing I tell you to that output”. Throughout the rest of the course we will be using ‘piping’ as it is easier to follow and code. To begin our piping journey, let`s repeat what we just did above but with pipes instead. 2.9.1 Activity - Pipe with pipes Copy and paste the below code into your script and run it. Here you are taking the data frame social_media and you are passing its contents to the filter() function. As you have piped social_media, you don’t have to tell filter() which data you want it to work with, as that is what is coming through the pipe. So you only need to provide the second argument, which tells the function how to filter the data. Check you get the same output as below. social_media %&gt;% #pass the values from social_media to the filter function filter(time_on_social == -999) #keep all rows that are equal to -999 ## id age time_on_social urban good_mood_likes bad_mood_likes followers ## 1 S24 14.4 -999 1 33.5 54.6 207.3 ## 2 S25 16.4 -999 1 13.5 41.3 190.0 ## polit_informed polit_campaign polit_activism ## 1 1.2 1.7 2.0 ## 2 1.3 1.9 2.2 Great. That was our first pipe! Now what we want to do is pass the result of the filtering to the count() function, so that we can find out how many rows of the data frame had missing data for the time_on_social variable. Update your code in your script so that it matches what you see in the code block below, and check you get the same result. social_media %&gt;% #pass the values from social_media to the filter function filter(time_on_social == -999) %&gt;% #keep all rows that are equal to -999 count() #count the number of remaining columns ## n ## 1 2 The answer should be satisfyingly the same as before. When coding, it is often a good idea to check you get the same answer by doing things in different ways. Then you know your code is doing the right thing and you can sleep well at night. Info: Piping is not friends with every function. Some functions will not accept inputs from pipes (no matter how nice they are!). This will become clearer as we code throughout this course. Now lets use a piping method to clean this data up by replacing -999 values with more R readable NA values. As mentioned, NA stands for ‘Not Available’ and is the standard way to represent missing data in R. We can replace the -999 values using the mutate() and na_if() functions from the tidyverse package. The mutate() function is used to alter or make new columns in a dataframe based on the conditions we specify and na_if() is used to replace given values with NA in a data frame (i.e. na_if() will look in each row to see if there is the value we have entered (-999). If it is TRUE that there is a -999, it na_if() will replace -999 with NA). We acknowledge that mutate() sounds quite dramatic, but all it really means is ‘take a column (or some columns), and do a thing to them and put the result in the column and give it the name listed on the left hand of the equals sign’. Copy and paste the below code into your script and run it. social_media_NA &lt;- social_media %&gt;% mutate(time_on_social = na_if(time_on_social,-999)) #mutate makes a column by altering rows. na_if replaces -999 with NA Now, very importantly, lets check that this worked by using the summary() function again. Re-use the summary() function in your code below the relevant comment. The one that says: ## Now run the summary() function again 2.10 Exporting Data It would be a good idea to save this dataset for future tutorials, so that we don’t have to replace -999 values with NA values every single time. We can do this with the write.csv() function from base R. This function takes a dataframe in R and saves it as a .csv file on your computer. Later, we can simply read that csv back into R, and it will already be cleaned. 2.10.1 Activity - Save the cleaned data Copy and paste this code into your script and run it. Then open the Output folder to check that it worked. You can do this by clicking on the folder in the Files tab in the bottom right section of RStudio. write.csv(social_media_NA, here(&quot;Output&quot;,&quot;PSYC2001_social-media-data-cleaned.csv&quot;)) #creates a csv file from the dataframe social_media_NA 2.11 Data visualization using ggplot() Visualizing data is a crucial step in data analysis. You should never run a statistical analysis without first visualising your data. It helps us understand the distribution of our data, identify patterns, and communicate our findings effectively. It also helps us identify whether the data are suitable for the analysis we want to perform, or whether some weird values remain that could influence the result of our statistical tests, and even worse, our interpretations! So, let’s look at some data! We’re going to start by visualising the time_on_social variable. To do this we will need to use the ggplot() function. This is the main function from the ggplot2 package which handily, comes for free as part of the tidyverse package. ggplot() provides the canvas of the graph you want to make. To make the basic canvas ggplot() requires two things: The data that you want it to plot. The variables to go on the x and y axes. Importantly, ggplot() only provides the canvas. It does not draw anything by itself. You have to add layers to the canvas created by ggplot() by using other functions that can create bars, points or lines ! 2.11.1 Activity - Starting a canvas in ggplot() First, let’s test what happens when we use ggplot() by itself. Complete the code in your script so that it matches the code block below and run it. social_media_NA %&gt;% ggplot(aes(y = time_on_social)) #ggplot uses aesthetic (aes()) to understand what # should be on the x and y axis You should now see a blank canvas with only the y-axis labelled. This will pop up in the ‘Plots’ pane, which is the same place as where you viewed the ‘Files’ pane. The canvas is blank because we have not added any layers to the canvas yet. Figure 2.9: A blank canvas awaits our data-viz artistry 2.11.2 Activity - Add a boxplot to the canvas So let’s add some layers to the canvas to make a graph. When using ggplot() we add layers by using the + operator. Thus, we add things to the canvas, one at a time. Here we add a geom_boxplot() layer which creates a boxplot for us. What is a boxplot, we hear you ask? A boxplot is a graph that shows the spread of data points where the lower part of the “box” represents the bottom quartile (where 25% of the data lies), the upper part of the box represents the upper quartile (where 75% of the data lies) and the middle of the box represents the median (the middle value). The “whiskers” (vertical lines) extend to the smallest and largest values not considered outliers. You can see more details on how to read boxplots here. Complete the code in your script so that it exactly matches the below, and then run it. Note there is a warning that 2 non-finite values have been removed. That’s just ggplot telling us ’hey, you got two NAs in here I had to take out. Did you know that?` Yes you did, for you put them there. social_media_NA %&gt;% ggplot(aes(y = time_on_social)) + #ggplot uses aesthetic (aes()) to map axes. geom_boxplot() # adds a boxplot layer to the canvas ## Warning: Removed 2 rows containing non-finite outside the scale range ## (`stat_boxplot()`). 2.11.3 Activity - tidy up the x-axis Ok, that’s a start, but there are a few things we can do to make this graph look better. First, the x-axis is a continuous numeric scale (i.e. values on x range from -0.4 to 0.4, with the boxplot sitting on the value x=0). However, we only have 1 category on the x-axis, so we can tell ggplot() to treat the x-axis as a discrete categorical by using the scale_x_discrete() function. Update your code in your script by adding the scale_x_discrete() function. This is adding another layer to your plot. Run the code again. social_media_NA %&gt;% ggplot(aes(y = time_on_social)) + #ggplot uses aesthetic (aes()) to map axes. geom_boxplot() + #creates a boxplot scale_x_discrete() #this tells ggplot that the x-axis is categorical. ## Warning: Removed 2 rows containing non-finite outside the scale range ## (`stat_boxplot()`). Ok, that is starting to look better. But let’s do one more thing. We can add labels to the axes using the labs() function. 2.11.4 Activity - add some labels Update your code in your script by adding the ylab() function. Make sure your code now exactly matches what is below. Run the code again to see what you get. social_media_NA %&gt;% ggplot(aes(y = time_on_social)) + #ggplot uses aesthetic (aes()) geom_boxplot() + #creates a boxplot scale_x_discrete() + #this tells ggplot that the x-axis is categorical. labs(y=&quot;Time on social media (hours)&quot;) #adds a y-axis label ## Warning: Removed 2 rows containing non-finite outside the scale range ## (`stat_boxplot()`). There, that’s much better! Warning: We receive a warning here (rather than an error) because ggplot() is able to recognise and remove ‘NA’ values. Be careful as not all R functions are able to do this. Question: What approximately is the median value? The lower quartile? The upper quartile? Is there another way that we could get this information in a more exact form ? Discuss this with your deskmate and your tutor. 2.11.5 Activity - More data-viz, more better - Creating a histogram in ggplot() ggplot() can be customised with so many other functions that we have not shown here to make truly beautiful looking plots. We will be learning how to do this throughout the next few weeks. For now lets see if you can put some of the skills you have learned so far to good use. See if you can work out how to make a histogram of the time_on_social variable using the function geom_histogram(). Note that a histogram is a graph that shows the distribution of a single numeric variable by dividing the data into bins and counting how many data points fall into each bin. The data is plotted along the x-axis, whereas the boxplot plotted the data along the y-axis. There, no more hints! Hint: You will only need to provide an x variable this time ! OK, one more hint. Complete the below code in your script. We have been a little cheeky and have not given you the answer this time. social_media_NA %&gt;% ggplot(aes(x = )) + #ggplot uses aesthetic (aes()) to map axes. geom_histogram() + #creates a histogram labs(x = &quot;Time on social media&quot;, y = &quot;Density&quot;) + #short for &quot;labels&quot;, use to label axes and titles. theme_classic() #changes the theme of the plot to a classic theme. makes it prettier! Question: What conclusions would you draw about the shape of the data, given your histogram? Please discuss with your deskmate and tutor. 2.12 You are Free! Well done! You have completed everything you need to for this week. If you have finished in a record time please consult with your tutor about what to do next. Otherwise we will see you next lab! Also, make sure to save your script when you’re done. This is your record of what you’ve done today, and you can refer back to it later. (One more hint for the road: you will refer back to it in future labs). Figure 2.10: Good work. You are done. Take a break. 2.13 ⭐ Bonus exercises Coding is just like solving logical puzzles, like Sudoku or Hexcells. If you like logical puzzles and want to try more, here are some bonus exercises (for fun, if that’s your sort of thing). 2.13.1 Bonus exercise - a bit more wrangling Create a new variable Use the mutate() function to calculate the difference between good_mood_likes and ’bad_mood_likes` for each participant. What does a positive vs negative value indicate? Filter and summarise Flex your filtering skills, and crack a new function Filter to get participants who spend more than 2 hours on social media per day, and pipe to the summarise() function to compute their average number of followers. There is extra help on the summarise() function here, where there are good examples on how to use the function. A good way to use Gen AI to help you learn to code is to ask it to explain a function does and to give you an example of how to use it. You can adapt the code it gives you as an example, to suit your own purposes and check that it works. Health warning When learning from code generated by Gen AI, you should make sure you test each line of code to make sure you know what it is doing. Gen AI is very helpful in teaching you to code, if you harness it right. But it does also love to sneak in extra unneeded lines of code, and the odd bug here and there. Breaking the generated code down to test it bit by bit, as you have been doing throughout this tutorial, is the best way to test it, and to learn from it. 2.13.2 Bonus exercise - getting to grips with graphs Let’s jazz up our histograms a bit. Make a histogram of followers. Using the examples here (or by getting an explanation from Gen AI), adjust the bin width and colour of your histogram. For a bonus bonus, you could play with adding themes to your histogram. And a final, extra challenge! Challenge: Reproduce a figure from R Graph Gallery Pick a plot style from https://r-graph-gallery.com/ and apply it to your chosen variable in your data set. Question: What new aesthetic choices did you learn? 2.13.3 Extra notes for the curious The way we used the if() function to check packages are loaded (at the start of this script) is a little tricksy, but worth thinking about, because the mindset we used is very important for coding. require() is a function that will return a TRUE value if the named package (e.g. here) is already installed, and a FALSE if it is not. The ! operator negates what is returned - i.e. if here is not installed, then require() will return the value FALSE. However, we want to turn this into the value TRUE, so that if() will run the next bit of code to install the package here if required. So ! turns the answer FALSE into not FALSE, which means TRUE. "],["sec-testing-first-hypothesis.html", "Chapter 3 Testing our first hypothesis 3.1 Checking installation and loading packages 3.2 Developing our hypotheses 3.3 Loading our data ready for visualisation and analysis 3.4 Visualising is important 3.5 Descriptive statistics 3.6 Testing hypotheses manually 3.7 Testing hypothesis using a paired-sample t-test 3.8 You are Free! 3.9 ⭐ Bonus exercises", " Chapter 3 Testing our first hypothesis Today we are going to ask our first question and seek an answer from the data. We will get the data into shape so that it is in the right format for visualizing and analysing. Then we will run the analysis and learn the answer to our question. Welcome. You are now a research psychologist :) 3.1 Checking installation and loading packages As usual we first always check and load in our required packages. 3.1.1 Activity - Load packages using the library() function Your script contains the code to install the packages, if required. However, the bit that contains the code to load the packages using the library() function is missing. Add the 2 extra lines of code to your script, and then make sure everything runs just fine. # Check if packages are installed, if not install. if(!require(here)) install.packages(&#39;here&#39;) #checks if a package is installed and installs it if required. if(!require(tidyverse)) install.packages(&#39;tidyverse&#39;) library(here) #loads in the specified package library(tidyverse) 3.2 Developing our hypotheses Today we are going to address one of the key questions of the study about social media use – how does mood influence active social media use? Active social media use involves interacting with content (i.e. liking posts) rather than just observing posts. There is some evidence to suggest that passive social media use is associated with lower mood in adolescents, whereas active social media use is related to positive mood Dienlin &amp; Johannes, 2020. However, a lot of the existing evidence comes from self-report, rather than measuring social media behaviour directly. To address this question, the researchers used the participants’ metadata to count how frequently they liked posts. By cross-referencing this with the mood diary kept by each participant, the researchers were able to calculate the average number of likes per 10 minutes of use when participants were in a good mood, and when they were in a bad mood. Hopefully it’s clear by now that we will be addressing this question using the good_mood_likes and bad_mood_likes variables. Remember, these variables stand for the following: good_mood_likes – average number of likes made over 10 min during a good mood (from platform + diary) bad_mood_likes – as above, but during bad mood But you should know this, because you read the README.txt file already, right? :) 3.2.1 Activity - Defining our hypotheses Based on the information above, discuss and formulate hypotheses around the following, and add them as comments to your code. Should there be a difference in the number of likes between the mood conditions? What direction do you think this difference could be? Can you formulate an experimental hypothesis each way – i.e. good mood likes &gt; bad mood likes, and vice versa? What is the null hypothesis? Question: Discuss this with your neighbour and your tutor. Make sure you have clearly defined your hypotheses as a comment in your code before moving forward. 3.3 Loading our data ready for visualisation and analysis When performing an analysis for an assignment, thesis, or publication, you will typically always follow the same basic steps. You will: Load the data. Visualise the distributions to check the data look ok (not ok data has strange outliers, strange values, or strange distributions). Get descriptive statistics and check they make sense. Perform the inferential test. Create a figure that provides a visual summary of the key statistical result. Although you will always perform steps 1 through 5, you will typically only report steps 3, 4 and 5 in your write-ups. So let’s get started. 3.3.1 Activity - Loading in the data Last lab we were very smart and saved the cleaned version of the data as a new CSV file. This is what we will be using today. We put it in the Data folder for you already. Use the Files pane to check the cleaned dataset is in the Data folder. Remember to click the two dots next to the green arrow to get back to the top-level folder. First step! Let’s load the dataset PSYC2001_social-media-data-cleaned.csv. To do this we use the same read.csv() function combined with here(). Do you remember how to do this? Complete the following line of code in your script. Again, we have been cheeky and have left a little bit out. But you should be able to work it out. You can also refer back to Section 2.5.1 if you need to. social_media &lt;- read.csv(file = here(???,&quot;PSYC2001_social-media-data-cleaned.csv&quot;)) #reads in CSV files Now, we did save the data ourselves last week, so we know it is clean. To be sure, add a line of code to your script so you can get a summary of the data using the summary() function. You can check that the -999s are no longer there. If you don’t remember how to do this, then you can check what happened in Section 2.6.1. 3.4 Visualising is important We may have mentioned before that it’s incredibly important to visualise your data before running any statistical tests. This is because visualising your data can help you understand the underlying distribution of the data, identify any potential outliers or anomalies, and ensure that the assumptions of the statistical test you plan to use are met. Remember, it protects against “garbage in, garbage out”! Warning: It is generally bad practice to go straight from the raw data to the results of a statistical test without first visualising the data. Figure 3.1: Professors’ reaction when you don’t visualise data 3.4.0.1 Density plots are useful (and pretty) One great way to look at the distribution of your variables is by using a density plot. A density plot is a smoothed version of a histogram which allows us to understand what the full distribution might look like if we had all the data in the world. More information on that is here if you are interested. In order to make plotting easy we first have to wrangle our data a bit. Remember, data analysis is 90% wrangling, 10% analysis! Our data is in something called wideform format. When data is in wideform, it means that each participant has their own row, and each variable has a column. Visually, this is more digestible for us mere humans. Wideform data ## id good_mood_likes bad_mood_likes ## 1 S1 22.8 46.5 ## 2 S2 46.0 48.3 ## 3 S3 50.8 46.1 ## 4 S4 29.9 29.2 ## 5 S5 37.1 52.4 ## 6 S6 26.9 20.2 However, many functions in R, particularly those from the tidyverse package, require data to be in longform. (Fun fact: longform data is also referred to as ‘tidy’ or ‘narrow’ data, hence the tidyverse). In longform data, each variable has its own column, but each row represents a single observation. Because we have two observations from each participant (good_mood_likes and bad_mood_likes), then each participant takes up two rows of the longform data frame. So the longform version of our data looks like this: Longform data ## # A tibble: 6 × 3 ## id mood likes ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 S1 good_mood_likes 22.8 ## 2 S1 bad_mood_likes 46.5 ## 3 S2 good_mood_likes 46 ## 4 S2 bad_mood_likes 48.3 ## 5 S3 good_mood_likes 50.8 ## 6 S3 bad_mood_likes 46.1 See how the values from row 1 of the wideform data (22.8 and 46.5) are now on the first and second row of the longform data, in the likes column. As you can see, instead of having each subject’s good and bad mood likes in separate columns, we now have a single column for likes and a second column which indicates whether the likes were made in a good or bad mood. So each subject now has two rows in the dataset. This makes it much easier to plot, and to do many other things! Helpful fact You may notice that the printout of the longform data has a few extra details, compared to the wideform data. The longform data is called ‘A tibble’ which is a special type of data frame that comes from the tidyverse package. It basically means that you used the tidyverse to make a new data frame, and the tidyverse package turned it into a tibble for you. Tibbles have a few extra features that make them easier to work with, but they are still data frames at their core. We’ll be calling them data frames throughout this course. 3.4.1 Activity - Get back on the pipes! Its time to get back on the pipes! The code we use to get our data into longform takes the following steps: First we use the select() function to easily choose which columns we do (or don’t) want to keep in our dataframe. Here we keep only the columns “id”, “good_mood_likes” and “bad_mood_likes”. Run the following code in your script so you can see what the select function does. Note that we are not saving this as a new object, so it will just print to the console. social_media %&gt;% select(&quot;id&quot;,&quot;good_mood_likes&quot;,&quot;bad_mood_likes&quot;) # choose which columns we want keep in our dataframe Handy hint: this is a great way to check what each function does! Try adding one more column from the dataframe to the select() function. For example, try adding the age column. Doing simple tests like this is a great way to check a function is really, really doing what you think. Once you’ve done that, remove the extra column you added so that your code matches the code above. Now that we have the columns we want to work with, we use the pivot_longer() function. This function will do all the heavy lifting turning our data from wideform to longform. Phew! This is how the using the pivot_longer function looks. social_media %&gt;% select(&quot;id&quot;,&quot;good_mood_likes&quot;,&quot;bad_mood_likes&quot;) %&gt;% # choose columns pivot_longer(cols = ends_with(&quot;likes&quot;), names_to = &quot;mood&quot;, values_to = &quot;likes&quot;) #take columns ending with &quot;likes&quot; and move the column names into &quot;mood&quot; and column values into &quot;likes&quot; The pivot_longer() function takes three important arguments. The cols argument tells R which columns contain the key variables that we need to turn into longform. Here we use the ends_with() function to tell R to take all columns which end with “likes”, nifty! Note that we didn’t necessarily need to use the ends_with() function. We just did it because it’s handy. We could have used cols = c(\"good_mood_likes\", \"bad_mood_likes\") instead, which would have done the same thing. The names_to argument tells R what to call the new column which will contain the names of the columns we will be pivoting (i.e good or bad mood). Here we call this new column “mood”, as this is a good title for a column that will list whether the data were recorded when someone was either in a “good mood” or a “bad mood”. The values_to argument tells R what to call the new column which will contain the values from the columns we are pivoting (i.e the rate of likes). Here we call this new column “likes”, because, erm, it contains the rate of likes. Highlight and run this code in your script to see the work of pivot_longer()! Remember Running code bit by bit to see what results you get is absolutely the best way to learn and understand what code is doing. Now that you have seen what select() and pivot_longer() actually do, you are perfectly placed to put them together and assign the results to a new object, so that you can use the results of your deft wrangling for other purposes. Now, very important, assign the ouput of your nifty coding work to a new object so that you can do impressive things to the results, like making density plots! Amend the above code in your script so that your data frame is saved to an object called social_media_likes. Then run the code to create the new object. If you need a reminder on how to assign things to objects, take a quick peek back at Section 1.8.1. Once you have assigned the data frame to an object called social_media_likes, make sure that the data frame assigned to the object looks as you would expect, by using the head() function. Complete the code in your script so that it looks exactly like below, and run it to check the result is the same as you see here. head(social_media_likes) ## # A tibble: 6 × 3 ## id mood likes ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 S1 good_mood_likes 22.8 ## 2 S1 bad_mood_likes 46.5 ## 3 S2 good_mood_likes 46 ## 4 S2 bad_mood_likes 48.3 ## 5 S3 good_mood_likes 50.8 ## 6 S3 bad_mood_likes 46.1 Checking your results like this is one of the key ways to make sure you don’t introduce bugs to your code, as you write your scripts. It takes time to get comfortable changing data from wideform to longform. For now, if you know the difference between the two, and you were able to follow the code above, then you are doing great! 3.4.2 Activity - start plotting Now, we can have done the wrangling (90%), we can get onto visualising and analysis! We get a density plot in R by using the geom_density function with ggplot(). To do this, we need to add a bit more information to the ggplot() function, compared to when we created a boxplot in Section 2.11.1. To make a density plot we need to tell it what variable we want to plot on the x-axis (likes), and we also want to tell it to use different density plots with different colours for the different moods. We tell ggplot() these things by calling the aes() function. You can think of aes() as setting the aesthetics of the plot. We tell aes() that we want to group the data by mood, and to fill in the density plots with different colours, depending on which mood is being plotted. Run the below code in your script to see what happens. social_media_likes %&gt;% ggplot(aes(x = likes, group = mood, fill = mood)) + # set canvas aesthetics geom_density() # use the data to draw a density plot So…this is OK. But some things are left to be desired. It would be nice if good_mood_likes didn’t occlude bad_mood_likes, because we want to see how both distributions look. Amend your code so that the density plots are semi-transparent. You can do this by adding an alpha argument inside the geom_density() function. You can think of alpha as a value that tells you how transparent something should be. Set alpha = 0.5 to make the plots 50% transparent. Make your code match what you see below and run it to get the sweet results. social_media_likes %&gt;% ggplot(aes(x = likes, group = mood, fill = mood)) + # set canvas aesthetics geom_density(alpha=0.5) # use the data to draw a density plot and make it 50% transparent Ahhh, that’s better! Then the last step, if you are striving for data viz beauty, is to add the classic theme (or another, if that’s your jam) to your code. Add the theme_classic() function to your code to give it a classic theme. Run the code to see the results. social_media_likes %&gt;% ggplot(aes(x = likes, group = mood, fill = mood)) + geom_density(alpha=0.5) + theme_classic() #themes can be provided to ggplot which give it a bunch of aesthetics to change. One of these is theme_classic For future you: a whole world of glorious movie inspired themes await you here. Now save your figure to your Output folder, by using the Export button on the Plots Pane. 3.4.3 Activity - So you’ve visualised your data, now what? Great, we’ve visualised the data! So, now what? I hear you cry? We visualised the data so we could check the following things before running our statistical test: 1. Are the distributions of likes in each mood roughly normal? 2. Are there any obvious outliers that might affect the results of our statistical test? What do you think? Make a comment in your code in answer to both these questions. Discuss your comments with your tutor if you are unsure. 3.5 Descriptive statistics Before we can move onto conducting t-tests, the next step is to understand the descriptive statistics. For the data we are looking at the most relevant descriptive statistics are the mean and standard deviation. This is because we want to conduct a t-test to compare the average likes in different moods. The t-test asks if the difference between the means is larger than we would expect by chance, given the variability in the data (i.e. the standard deviation). That’s why we need to know the mean and standard deviation of likes in each mood. tidyverse to the rescue! We can easily get this information in R by using the summarise() function. The summarise function will take a dataframe and calculate summary statistics for it, and we get to define what summary statistics we want. The power! Because we want to know the mean and standard deviation of likes in each mood, we need to use the group_by() function to tell R to split the data by mood first. 3.5.1 Activity - get descriptive! Amend the following code in your script so that it matches what you see below. Then run it to get the mean and standard deviation of likes in each mood. social_media_likes %&gt;% group_by(mood) %&gt;% #split the data by mood summarise(mean = mean(likes), sd = sd(likes)) #calculate the mean number of likes ## # A tibble: 2 × 3 ## mood mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bad_mood_likes 49.8 17.2 ## 2 good_mood_likes 43.0 16.1 We can also save this as a new data frame if we want to use it later. Amend your code so that the results are assigned to a new data frame object called social_media_descriptives. You can use the same method as when you assigned your data frame to social_media_likes. Then run the code to create the new data frame. Question: What are the mean number of likes in each different mood? What is the standard deviation? How do they compare to what you expected when you made your hypotheses? Make a note in your comments underneath your original statement of the hypotheses. 3.6 Testing hypotheses manually Now that we have had a look at the structure and descriptive statistics of our data we can have a go at using a t-test to compare the number of likes participants made in a good and bad mood. We could of course do this manually, by hand (as we have in the statistics tutorials). We have shown this below using the first 10 values for good and bad mood likes. Figure 3.2: t-test table Thankfully, we have long since past the stone (pen) age so it is no longer necessary to do this by hand. We can get computers to do this for us! Figure 3.3: Handwritten statistiscs be like 3.7 Testing hypothesis using a paired-sample t-test We can very easily calculate a t-test with R using the t.test() function which comes as part of baseR. Have a look at the help for this function using the ? syntax, much as you did in Section 1.5. We recommend you type your request for t.test help info directly into the console. You should now see some helpful information in the Help pane. Figure 3.4: Handy t-test help We can see that t.test() takes different arguments which are important for the way it handles the data. What values you provide to these arguments is dependent on what kind of test you want to conduct. 3.7.1 Activity - Conducting a paired t-test We are going to go through the arguments we need to perform a paired sample t-test. We can see from the help info that we should input a set of data values to an argument called x and another set of data values to an argument called y. You can think of x and y as stand-in names for the two variables we want to compare, which are good_mood_likes and bad_mood_likes. Now the t.test function is from base R, and now you get to meet one of the peculiarities of base R. The x and y arguments expect your data to be in wideform format –But we did all that work getting it into longform, I hear you cry– Yes, we did. But remember, that allowed us to easily compute descriptive statistics and visualise the data. Luckily for us, we also have the original wideform data saved in the social_media dataframe. So we will use that to compute our t-test. To do that, we need to know about a magical operator in R which is $. The $ operator allows us to access the values in specific columns in a data frame. For example, if we wanted to access the good_mood_likes column in the social_media data frame, we would use social_media$good_mood_likes. This tells R to look in the social_media data frame and get the stuff that lives in the column called good_mood_likes. Complete the line in your script so that it matches below, and then run it to see what happens. social_media$good_mood_likes #access the good_mood_likes column in the social_media dataframe You should see all the numbers from the good_mood_likes column, printed to the dataframe. That’s exactly what the t.test() function needs to see to do its work. Complete the code in your script so that it matches what you see below. But don’t run it yet! There are some more things we need to tell t.test() before we can get it to do what we want. t.test(x=social_media$good_mood_likes, #the x argument gets the good_mood_likes data y=social_media$bad_mood_likes) #the y argument gets the bad_mood_likes data Now there is one more argument that we super care about setting when performing a t test. This is the paired argument. The paired argument tells R whether the two sets of data we are comparing are dependent (e.g. from the same people) or independent (e.g. from different people). We refer to dependent observations as “paired”. Here, we know that the good and bad mood likes are from the same people, so we need to set paired = TRUE. If we were comparing likes from two different groups of people, we would set paired = FALSE. Update the code in your script so that it exactly matches the below. Run it and check the output is the same as what we have here. t.test(x=social_media$good_mood_likes, #the x argument gets the good_mood_likes data y=social_media$bad_mood_likes, #the y argument gets the bad_mood_likes data paired = TRUE) #tell R that the data are paired (i.e. from the same people) ## ## Paired t-test ## ## data: social_media$good_mood_likes and social_media$bad_mood_likes ## t = -3.336, df = 59, p-value = 0.001474 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -10.881375 -2.721958 ## sample estimates: ## mean difference ## -6.801667 Now we have some answers! Use your new found knowledge of t-tests to interpret the output. What does it tell you? Does it refute the null hypothesis? Does it support what you thought? Write your interpretation of the result as a comment under the code you used to perform the t-test. Find in the output the 95% confidence intervals for the mean difference. Write them as a comment in your code and your interpretation regarding the possible size of the effect, in the context of our hypothesis. 3.8 You are Free! Well done you have finished for this week! Once you finish, please confirm with your tutor that you understand all the things. Figure 3.5: Admit it, that was some good hypothesis work 3.9 ⭐ Bonus exercises 3.9.1 Bonus Activity - Correspondence to one-sample t-tests If you have cracked all the activities above, well done, you are coding like a fiend. Time for bonus knowledge, if you seek it. Now that we have the output of a paired t-test we can compare it to a one sample t-test on difference scores. The results of both of these tests should be identical (this has been covered in the lectures). For those that haven’t attended the lectures, a) attend the lectures because if not you will miss out on so much more knowledge, and b) this is because a paired t-test is basically a one-sample t-test of matched difference scores (more on that here) Let’s conduct a one-sample t-test of the difference scores between good and bad mood likes. Use the mutate() function on the original social_media dataframe to create a difference score column - i.e. subtract good_mood_likes from bad_mood_likes (the code is below, in case you didn’t complete the bonus exercises from last lab). You should see something that looks like the below: social_media %&gt;% mutate(likes_diff = good_mood_likes - bad_mood_likes) #create a new column which is the difference between # good and bad mood likes ## X id age time_on_social urban good_mood_likes bad_mood_likes followers ## 1 1 S1 15.2 3.06 1 22.8 46.5 173.3 ## 2 2 S2 16.0 2.18 1 46.0 48.3 144.3 ## 3 3 S3 16.8 1.92 1 50.8 46.1 76.5 ## 4 4 S4 15.6 2.61 1 29.9 29.2 171.7 ## 5 5 S5 17.1 3.24 1 37.1 52.4 109.5 ## 6 6 S6 15.7 2.44 1 26.9 20.2 157.5 ## polit_informed polit_campaign polit_activism likes_diff ## 1 2.3 3.2 3.6 -23.7 ## 2 1.6 2.2 2.6 -2.3 ## 3 1.9 2.7 3.0 4.7 ## 4 1.6 2.3 2.6 0.7 ## 5 2.0 2.9 3.3 -15.3 ## 6 2.4 3.4 3.9 6.7 Now, we’re going to need to save that dataframe to an object called social_media_diff, so that we can use it in the t.test() function. Update the code you just ran so that it starts with social_media_diff &lt;- and run it to save the dataframe. Now that we have correctly calculated and saved our difference score we can use the t.test() function to perform our one-sample t.test. Copy the below line of code into your script and run it. Compare the output to the output of the paired t-test you did earlier. Are they the same? t.test(x=social_media_diff$likes_diff) # putting in only an x argument makes this a one-sample t-test ## ## One Sample t-test ## ## data: social_media_diff$likes_diff ## t = -3.336, df = 59, p-value = 0.001474 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -10.881375 -2.721958 ## sample estimates: ## mean of x ## -6.801667 Ahhhh, its so beautiful when you get consistent results! 3.9.2 Bonus Activity - visualising differences in paired data There are lots of ways to visualise paired data. One great way is to use a spaghetti plot. A spaghetti plot shows the individual data points for each participant in both conditions, and connects them with a line. This allows us to see how each participant’s likes changed between good and bad mood. i.e. we can see how consistent the effect is - does everyone’s line go up or down, or just for some people? Here is the basic code for a spaghetti plot: social_media_likes %&gt;% ggplot(aes(x=mood, y = likes, group=id)) + geom_line() + geom_point() Copy the code to your script and run it. Can you write a comment in your code interpreting what each line of the code is doing? The code above produces a very basic spaghetti plot. We can improve it by adding some aesthetics. Can you change the code so that the points are bigger? So that the lines are a bit lighter? Helpful hints: start by looking at the examples in the help for geom_line() and geom_point(), or ask your favourite LLM how you would adapt the above code to achieve these objectives. How about adding a theme to make it prettier? What else can you think of? Is there a better way to present the data? Use the R Graph Gallery for inspiration. "],["testing-for-differences-between-groups.html", "Chapter 4 Testing for differences between groups 4.1 Checking installation and loading packages 4.2 How does where you live impact how you use social media? 4.3 Wrangling our data 4.4 Visualising our data 4.5 Next step: descriptive statistics 4.6 Independent samples t-test 4.7 Writing up results and conclusions 4.8 You are Free! 4.9 ⭐ Bonus exercises", " Chapter 4 Testing for differences between groups This week we will be learning how to conduct an independent samples t-test. This is a statistical test that allows us to compare the means of two independent groups on a continuous outcome variable. According to the 90/10 theorem, we will spend 90% of our time wrangling and visualising our data, and 10% of our time actually conducting the t-test. 4.1 Checking installation and loading packages As usual we first always check and load in our required packages. 4.1.1 Activity - load packages using library() Just like in Chapter 3, your script contains the code to install the packages, if required. Add the two lines of code that use the library() function to load here and tidyverse to your own script. Run all the code. # Check if packages are installed, if not install. if(!require(here)) install.packages(&#39;here&#39;) #checks if a package is installed and installs it if required. if(!require(tidyverse)) install.packages(&#39;tidyverse&#39;) library(here) #loads in the specified package library(tidyverse) 4.2 How does where you live impact how you use social media? In Chapter 3, we performed an analysis to learn how mood impacts active social media behaviour. However, that is not the only factor that influences social media use. For example, Sapienza et al (2023) found that people in rural areas are more likely to use their smartphone for social media and gaming, whereas urban dwellers are more likely to use their phone for navigation and business. However, we do not know if people living in urban and rural areas engage with social media differently, regardless of how long they spend on their chosen platforms. Today we will address this question using the urban, good_mood_likes, bad_mood_likes, and followers variables. Remember, these variables stand for the following: urban – urban (1) or rural (2) area (based on postcode density) good_mood_likes – average number of likes made over 10 min during a good mood (from platform + diary) bad_mood_likes – as above, but during bad mood followers – average number of followers across platforms 4.2.1 Activity - Formulate your research question What do you think? Will urban and rural dwellers engage differently with social media? Why might you go on social media if you were somewhere rural compared to urban? Will there be a difference in the number of likes made by people living in urban vs rural areas? Or in the number of followers people have in urban vs rural areas? Extra info: We are going to average over the effect of mood, so we do not need to include mood in our predictions about likes. Write down your answers to the these questions as a comment in your script. Question: What are the null and alternate hypothesis for your research questions (you should have one for ‘likes’ and one for ‘followers’). Write these as a comment in your script. 4.2.2 Activity - Load in data and check it Today we will be averaging across mood to get the number of likes for urban and rural dwellers. This means we first need to create a new variable called likes which is the average of the likes in a good and bad mood. We first load in ourPSYC2001_social-media-data-cleaned.csv dataset. Copy and paste this code into your script and run it to load in the data. social_media &lt;- read.csv(file = here(&quot;Data&quot;,&quot;PSYC2001_social-media-data-cleaned.csv&quot;)) #reads in CSV files Lets double check its the data we think it is by using the head() function. See that we’ve added in an argument to say we want to see the first 10 lines of the data. Amend your code in your script to look at the number of rows your very own eyes want to see. head(social_media, 10) # you can even say how many lines you want to see! Try changing the number, and see what happens. ## X id age time_on_social urban good_mood_likes bad_mood_likes followers ## 1 1 S1 15.2 3.06 1 22.8 46.5 173.3 ## 2 2 S2 16.0 2.18 1 46.0 48.3 144.3 ## 3 3 S3 16.8 1.92 1 50.8 46.1 76.5 ## 4 4 S4 15.6 2.61 1 29.9 29.2 171.7 ## 5 5 S5 17.1 3.24 1 37.1 52.4 109.5 ## 6 6 S6 15.7 2.44 1 26.9 20.2 157.5 ## 7 7 S7 19.7 1.46 1 14.8 35.1 166.9 ## 8 8 S8 18.6 1.52 1 26.0 35.8 109.6 ## 9 9 S9 19.6 1.92 1 6.5 12.2 253.4 ## 10 10 S10 15.5 2.10 1 45.7 32.8 77.2 ## polit_informed polit_campaign polit_activism ## 1 2.3 3.2 3.6 ## 2 1.6 2.2 2.6 ## 3 1.9 2.7 3.0 ## 4 1.6 2.3 2.6 ## 5 2.0 2.9 3.3 ## 6 2.4 3.4 3.9 ## 7 1.7 2.4 2.7 ## 8 1.6 2.2 2.6 ## 9 1.5 2.1 2.4 ## 10 2.2 3.1 3.5 4.3 Wrangling our data There are a couple of things we need to do to get our data into shape. The first is to create our new variable likes which is the average of good_mood_likes and bad_mood_likes. The second is to define the urban factor correctly. More on that to come! 4.3.1 Activity - Creating a new likes variable We need to create a new variable called likes which is the average of good_mood_likes and bad_mood_likes. To do this, we are going to use the mutate() function from the tidyverse package. You can think of using mutate() as a way to create a new column in your data frame, using information from other columns. To use mutate() we need to use the pipe operator %&gt;% which we also used in Section 3.4.1. The pipe operator takes the output of one function and uses it as the input for the next function. This is very useful when we want to do multiple things to a data frame in a single line of code. We are going to use the pipe operator to take the social_media data frame and then use mutate() to create our new variable likes. Because we want the average likes, we will add good_mood_likes and bad_mood_likes together and then divide by 2. Because this is a new way of using code, we need to carefully design a test that tells us if our code is working as it should. This is a very important part of coding. In this case, the test is very easy to design. We can manually calculate the first few values of likes that our code should produce. Then, if our code does indeed produce those values, we know exactly what our code is doing. Using the console (or in your head), calculate what values you should get for the first three rows of the likes column, when taking the average of bad_mood_likes and good_mood_likes. Add these values as comments to your script. Now, get the whole column of averages by running the following line of code in your script. social_media_likes &lt;- social_media %&gt;% mutate(likes =(bad_mood_likes + good_mood_likes)/2 ) # creates a new variable called likes which is the average of bad_mood_likes and good_mood_likes ## X id age time_on_social urban good_mood_likes bad_mood_likes followers ## 1 1 S1 15.2 3.06 1 22.8 46.5 173.3 ## 2 2 S2 16.0 2.18 1 46.0 48.3 144.3 ## 3 3 S3 16.8 1.92 1 50.8 46.1 76.5 ## 4 4 S4 15.6 2.61 1 29.9 29.2 171.7 ## 5 5 S5 17.1 3.24 1 37.1 52.4 109.5 ## 6 6 S6 15.7 2.44 1 26.9 20.2 157.5 ## 7 7 S7 19.7 1.46 1 14.8 35.1 166.9 ## 8 8 S8 18.6 1.52 1 26.0 35.8 109.6 ## 9 9 S9 19.6 1.92 1 6.5 12.2 253.4 ## 10 10 S10 15.5 2.10 1 45.7 32.8 77.2 ## polit_informed polit_campaign polit_activism likes ## 1 2.3 3.2 3.6 34.65 ## 2 1.6 2.2 2.6 47.15 ## 3 1.9 2.7 3.0 48.45 ## 4 1.6 2.3 2.6 29.55 ## 5 2.0 2.9 3.3 44.75 ## 6 2.4 3.4 3.9 23.55 ## 7 1.7 2.4 2.7 24.95 ## 8 1.6 2.2 2.6 30.90 ## 9 1.5 2.1 2.4 9.35 ## 10 2.2 3.1 3.5 39.25 You can see that mutate() has created an extra column called likes. Check that the values in the first three rows of the likes column match what you calculated and added to your comments. Talk to your tutor if they do not match. Tip: Manually checking that a function has done what you think it should is a good habit to get into. It will help you catch bugs early on. Now, this data frame has all we need for our analysis, and a lot more! Let’s make things simpler for ourselves by only keeping the variables we need. We can do this by once again using the pipe operator, along with the select() function. Update the code in your script so that it looks exactly like this: social_media_likes &lt;- social_media %&gt;% mutate(likes =(bad_mood_likes + good_mood_likes)/2 ) %&gt;% # creates a new variable called likes which is the average of bad_mood_likes and good_mood_likes select(id, urban, likes, followers) # selects only the specified columns from the data frame Check that the code did indeed select only the variables we want, by using the head() function to check what the new data frame called social_media_likes looks like. If you need help, there is an example for how to use the head() function in Section 2.6.1. 4.3.2 Activity - Defining factors Now that we have this data frame object it is important to check that R can understand the data properly. Lets use the str() function that we learned about in Section 2.6.1 to examine what R thinks about each column. Complete the code in your script and run it, to make sure you get the same results. str(social_media_likes) #provides a summary of the data structure. ## &#39;data.frame&#39;: 60 obs. of 4 variables: ## $ id : chr &quot;S1&quot; &quot;S2&quot; &quot;S3&quot; &quot;S4&quot; ... ## $ urban : int 1 1 1 1 1 1 1 1 1 1 ... ## $ likes : num 34.6 47.1 48.5 29.5 44.8 ... ## $ followers: num 173.3 144.3 76.5 171.7 109.5 ... What have we learned? We can see that R thinks that urban contains integers (int), i.e. R thinks that urban is a column of real numbers containing 1s and 2s. But urban is not a numeric variable, it is actually a factor (categorical variable). If you remember from the README.txt file (which you read already, right? :)), 1 stands for urban and 2 stands for rural. We need to tell R this, so that it can understand how to work with this variable. To do this, we can use the factor() function, which converts a variable to a factor. We also need to tell R that 1 means urban and 2 means rural. Because we need the result to apply to every single 1 and 2 in rural, we need to use the factor() function along with a function that says “hey, let’s apply this to the whole column”. Remember how we use the mutate() function to make a new column? We can also apply the mutate() function to an existing column, as what ever you give it will be applied to every row of whichever column you name when using the function. The mutate() function is coming in handy today! Run the line of code in your script that matches this. social_media_likes &lt;- social_media_likes %&gt;% mutate(urban = factor(urban, levels=c(1,2), labels=c(&#39;urban&#39;, &#39;rural&#39;))) #changes the urban variable to a factor with levels urban and rural Now we want to test that this code did what we wanted it to do. Use the str() function again to check that urban is now a factor. str(social_media_likes) ## &#39;data.frame&#39;: 60 obs. of 4 variables: ## $ id : chr &quot;S1&quot; &quot;S2&quot; &quot;S3&quot; &quot;S4&quot; ... ## $ urban : Factor w/ 2 levels &quot;urban&quot;,&quot;rural&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ likes : num 34.6 47.1 48.5 29.5 44.8 ... ## $ followers: num 173.3 144.3 76.5 171.7 109.5 ... Excellent! We can see that R now thinks that urban is a factor with 2 levels: urban and rural. Our data is now in a format that we should be able to easily visualise it and conduct our statistical tests. 90% done :) Figure 4.1: The math checks out. We promise. 4.4 Visualising our data If you remember from Section 3.4.3, we learned that we need to visualise our data to check if the distributions look roughly normal, and to check there are no obvious outliers or strange values that will impact our analysis. Today, we are going to examine our variables using histograms. Just as we did in Section 2.11.5. We have copied the code from Section 2.11.5 below, and have added a few options into the geom_histogram() function so that you can get a nice looking histogram. This code is also in your script. 4.4.1 Activity - Adapting our previous code to make new histograms Your challenge, should you choose to accept it*, is to change the code in your script so that you instead make a histogram for likes (x=) using the social_media_likes data frame (and not the social_media_NA data frame). Also remember to adjust the axis labels (the x= and y= arguments for labs())! Change the below code in your own script to make the histogram for likes. *You have to accept it. Determinism beats free will. ## health warning. This is old code that needs to be adapted for current purposes! social_media_NA %&gt;% ggplot(aes(x = )) + #ggplot uses aesthetic (aes()) to map axes. geom_histogram(binwidth=10, col=&quot;black&quot;, fill=&quot;seagreen&quot;) + #creates a histogram with blue fill, black borders, and a binwidth of 10 labs(x = &quot;Time on social media&quot;, y = &quot;Density&quot;) + #short for &quot;labels&quot;, use to label the axes. theme_classic() #changes the theme of the plot to a classic theme. makes it prettier! You should get a histogram that looks something like this: What would you say about this data, now you have looked at the histogram? Does it look normally distributed? Are there any outliers? Copy and paste the code you used to make the histogram for likes and make the teeny changes required so that you get a histogram for followers instead. Go on, you are a coder, and you are strong. Question: Does followers look normally distributed to you? Why might the data be shaped how it is? Indeed, the followers variable looks different to the likes variable. This is not too surprising. A fewer number of people in the sample have a huge number of followers, and the rest have a more modest number. Just like in real life. Random sampling works! Ahhh, science. Because the followers variable is not normally distributed, we will need to be a bit cautious when interpreting the results of our t-test later on. But we can still proceed, as the t-test is quite robust to violations of normality. You would just make sure to mention this in your write-up, so that your readers also know. However, we would reconsider this choice if followers contained a massive outlier, such as if we happened to sample someone with 10 million followers. 4.5 Next step: descriptive statistics Now that we know what shape our data is in, and when we should exercise caution, we can move on to generating some descriptive statistics of our key variables. In Section 3.5.1 we learned how to summarise our data to get descriptive statistics. Now you’ll get to see some more of the joy of coding - once you have written a bit of code that does something you need, its very easy to adapt it to do something else you need. 4.5.1 Activity - Adapt the code to get the mean and standard deviations for likes and followers Here is the code we wrote last week to get descriptive statistics of the rate of likes, grouped by mood: social_media_likes %&gt;% group_by(mood) %&gt;% #group the data by mood summarise(mean = mean(likes), sd = sd(likes)) #calculate the mean number of likes We want to use this code as a template for ourselves, so that we can get the mean and standard deviation for likes and followers, this time grouping by urban. You will find the below code in your script. Complete it to get the descriptive statistics for likes and followers. social_media_descriptives &lt;- social_media_likes %&gt;% # save to new object called social_media_descriptives group_by(???) %&gt;% # group the data by urban summarise( mean_followers = mean(followers), # calculate the mean number of followers for urban and rural groups separately mean_likes = mean(likes), # same for likes sd_followers = sd(???), # calculate the sd of followers sd_likes = sd(???) # calculate the sd of likes ) social_media_descriptives Check that your output for social_media_descriptives looks like this: ## # A tibble: 2 × 5 ## urban mean_followers mean_likes sd_followers sd_likes ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 urban 144. 40.8 62.0 14.5 ## 2 rural 106. 52.1 40.9 12.7 Look at the mean values you have generated for urban and rural dwellers. Are they what you expected? Do they fit with your predictions from earlier? 4.6 Independent samples t-test Now we are down to the very last few percents of our 10% of analysis efforts. Let’s perform some independent samples t-tests to see if there are statistically significant differences between urban and rural dwellers in terms of their likes or followers. 4.6.1 Activity - Performing independent samples t-tests In Section 3.7.1 we learned how to conduct a paired samples t-test. To do this, we had to deal with niggly base R demands and use the wideform data instead of our carefully wrangled longform data. Fortunately, the t.test() function is a bit more forgiving when it comes to independent samples t-tests, and we can use our longform data combined with a nifty thing that base R knows about, which is formulas. Huzzah. This is how we use the formula method in t.test(). The syntax is t.test(DV ~ IV, data = data frame). So, for our data, we want to test if the dependent variable (DV) likes differs by the independent variable (IV) of group that is coded in urban, using the social_media_likes data frame. Note that you don’t need to set the paired argument to FALSE, as the t.test() function assumes this whenever you use the formula method. So never use the formula method for a paired samples t-test. That would be a disaster. Run the below line of code in your script, to check you get the same output. t.test(likes~urban, data=social_media_likes) # conducts an independent samples t-test to see if likes differ by urban/rural status ## ## Welch Two Sample t-test ## ## data: likes by urban ## t = -3.2184, df = 56.996, p-value = 0.002129 ## alternative hypothesis: true difference in means between group urban and group rural is not equal to 0 ## 95 percent confidence interval: ## -18.338983 -4.271017 ## sample estimates: ## mean in group urban mean in group rural ## 40.78833 52.09333 What does the output of the independent t-test tell you about the differences between urban and rural dwellers and how they actively use social media? Is it what you expected when you formulated your hypothesis? Write a comment in your code that interprets your result in the context of your hypothesis. What do the 95% CIs tell you about the mean difference between urban and rural dwellers in terms of likes? Write a comment in your code that interprets this. Is there a piece of extra information you would like to know about the difference between groups that is not provided in the output for the independent t-test? If you are unsure, ask your tutor. If you are, calculate the missing piece of information and add it as a comment in your code. Now complete the following line of code in your own script, to test if followers differ by urban. t.test(???~urban, data=social_media_likes) # conducts an independent samples t-test to see if followers differ by urban/rural status Check your output with what you see below to make sure you got it right. Also, make sure to interpret what the output is telling you! ## ## Welch Two Sample t-test ## ## data: followers by urban ## t = 2.8182, df = 50.234, p-value = 0.00689 ## alternative hypothesis: true difference in means between group urban and group rural is not equal to 0 ## 95 percent confidence interval: ## 10.98893 65.49107 ## sample estimates: ## mean in group urban mean in group rural ## 143.8767 105.6367 Figure 4.2: We have all been there. 4.7 Writing up results and conclusions This is how we would write up the results of an independent samples t-test. Results: An independent samples t-test indicated that the mean number of likes was greater for rural (M = 52.1 , SD = 12.7) compared to urban dwellers (M = 40.8 , SD = 14.5) and this difference was statistically significant (t(58) = 3.22, p = 0.002). Conversely, the mean number of followers was greater for urban (M = 143.9, SD = 62.0) than for urban dwellers (M = 105.6, SD = 40.9) which was statistically significant (t(58) = 2.82, p = 0.007). 4.7.1 Activity - Visualising group differences If you have made it this far with time to spare, then kudos. Here are some extra skills that will be very helpful when you need to present results relating to group differences. Earlier, we looked at the overall histograms for likes and followers. But when presenting our findings in a results section, what the reader really wants to see is how the groups differ from each other. One effective way to do this is to make a grouped boxplot. See here for more details on boxplots. Let’s make a boxplot, showing the likes data split by urban and rural groups. We are going to use a couple of new functions here (geom_boxplot(), scale_fill_manual()), which we explain in the comments below. social_media_likes %&gt;% ggplot(aes(y = likes, group=urban, fill = urban, x = urban)) + # here we are telling ggplot that we will be putting likes # on the y axis, that we will fill in our boxplots with colour using the urban factor, and that urban will go on the x-axis geom_boxplot() + # this function creates a boxplot labs(x = &quot;Living Area&quot;, y = &quot;Average Rate of Likes&quot;) + # here we use labs() to label our axes. scale_fill_manual(values = c(rural = &quot;plum&quot;, urban = &quot;cyan2&quot;)) + #manually define the filled in colours of specific parts of a graph - see here for more R colours: https://r-graph-gallery.com/42-colors-names.html theme_classic() Now we return to another critical part of coding. Copying and pasting what you have, and changing just a few bits. Copy and paste the code above and change it to make a boxplot for followers instead of likes. You will need to change what you give for the y argument in aes(), and the y-axis label. You can also change the colours if you want to. A list of colours that can be used can be found here.One you have finished, you should get something that looks like this: Last, we often want to save our figures as image files, so that we can include them in reports or presentations. This is very easy to do using the ggsave() function. First, you should save the plot you want to save to an object. Amend your boxplot code so that the output saves to an object, such as you see below. likes_bp &lt;- social_media_likes %&gt;% ggplot(aes(y = likes, group=urban, fill = urban, x = urban)) + geom_boxplot() + # this function creates a boxplot labs(x = &quot;Living Area&quot;, y = &quot;Average Rate of Likes&quot;) + # here we use labs() to label our axes. scale_fill_manual(values = c(rural = &quot;plum&quot;, urban = &quot;cyan2&quot;)) + theme_classic() Also amend your code for the followers boxplot so that it is saved to an object called followers_bp. Run the code and check the new object appears in the list in your Environment pane. Then, you can use the ggsave() function to save the plot to a file. The syntax is ggsave(\"filename.png\", plot = plot_name). You can change the file extension to save in different formats (e.g., .jpg, .pdf). We want to be extra smart and save the file to our Output folder, so we will use the here() function to specify the path. Copy and paste this code to your script, and run it. Then check the contents of the ‘Output’ folder. ggsave(here(&quot;Output&quot;, &quot;likes_boxplot.png&quot;), plot = likes_bp) # saves the likes boxplot to the Output folder as a PNG file Now copy and paste that, and change it so that you save your followers_bp plot instead of the likes_bp plot. 4.8 You are Free! Well done! This computing tutorial is now over. You have done your research. Figure 4.3: You can. You are a scientist now. 4.9 ⭐ Bonus exercises If that just wasn’t enough, then here are some extra coding challenges to defeat… 4.9.1 Bonus Activity - new variable! Create an extra variable called engagement which is the ratio of likes and followers. Then get the descriptive statistics for this new variable, grouped by urban. What can you learn about the distribution of this new variable? Can you analyse it by adapting the code from this session? 4.9.2 Bonus Activity - Visualising group means Before, we made boxplots to show the group differences between urban and rural dwellers. Boxplots are great, because they provide a good summary of the spread of the data. However, we make inference regarding the difference between group means, rather than the medians, and it is the group medians that the boxplots show. As the mean can be different to the median when the data are skewed, it can be helpful to add a point showing each group mean on top of the boxplots. Can you amend your code for your boxplots by using the stat_summary() function? 4.9.3 Bonus Activity - Tracking bugs Here we will have a go at identifying when code has not worked perhaps as we thought it would. This challenge is a little fiendish, but I am sure you can do it. Copy the following line of code into your script and run it. You should see the same data as below. Can you work out what went wrong with the new_likes variable? Can you write the code that proves what is wrong? social_media %&gt;% mutate(new_likes = mean(c(good_mood_likes, bad_mood_likes))) ## X id age time_on_social urban good_mood_likes bad_mood_likes followers ## 1 1 S1 15.2 3.06 1 22.8 46.5 173.3 ## 2 2 S2 16.0 2.18 1 46.0 48.3 144.3 ## 3 3 S3 16.8 1.92 1 50.8 46.1 76.5 ## 4 4 S4 15.6 2.61 1 29.9 29.2 171.7 ## 5 5 S5 17.1 3.24 1 37.1 52.4 109.5 ## 6 6 S6 15.7 2.44 1 26.9 20.2 157.5 ## 7 7 S7 19.7 1.46 1 14.8 35.1 166.9 ## 8 8 S8 18.6 1.52 1 26.0 35.8 109.6 ## 9 9 S9 19.6 1.92 1 6.5 12.2 253.4 ## 10 10 S10 15.5 2.10 1 45.7 32.8 77.2 ## polit_informed polit_campaign polit_activism new_likes ## 1 2.3 3.2 3.6 46.44083 ## 2 1.6 2.2 2.6 46.44083 ## 3 1.9 2.7 3.0 46.44083 ## 4 1.6 2.3 2.6 46.44083 ## 5 2.0 2.9 3.3 46.44083 ## 6 2.4 3.4 3.9 46.44083 ## 7 1.7 2.4 2.7 46.44083 ## 8 1.6 2.2 2.6 46.44083 ## 9 1.5 2.1 2.4 46.44083 ## 10 2.2 3.1 3.5 46.44083 Bug defeated? Achievement unlocked! "],["probing-statistical-relationships-with-correlation-analysis.html", "Chapter 5 Probing (Statistical) Relationships with Correlation Analysis 5.1 Checking installation and loading packages 5.2 Investigating correlational relationships 5.3 Building a composite score that measures political attitude 5.4 How would political attitudes relate to social media use? 5.5 Looking for lines 5.6 Conducting correlations 5.7 Testing the statistical significance of the correlation coefficient 5.8 Writing up results and conclusions 5.9 You are Free! 5.10 ⭐ Bonus exercises", " Chapter 5 Probing (Statistical) Relationships with Correlation Analysis This week, we are going to explore relationships between continuous variables, using correlation. We will take a look at how to combine composite variables into a single score, as this is something we often do in the pusuit of understanding the human psyche. We will also visualise relationships between variables using scatterplots, and conduct Pearson correlation analyses in R. 5.1 Checking installation and loading packages As usual we first always check and load in our required packages. This week, as usual, we will need the packages here and tidyverse. 5.1.1 Activity - write the code to load packages from the library Today, we are not giving you the code to copy and paste. You are ready to fly, and write it yourself now you have mastery of the codeship. You are welcome to decide yourself whether you would like to check if the packages are installed before you load them. Go to your script for the week ‘05_Correlation-analysis.R’ and add the code to load the packages here and tidyverse using the library() function. Write the code to check if the packages are installed if you wish. Make sure to run the code. 5.2 Investigating correlational relationships We have previously looked at how mood and location influence social media use. Now we’re going to look at the relationship between social media use, age and political activism. It has previously been shown that social media use has a positive relationship with levels of political activism in other countries such as Jordan (Alodat et al, 2023). However, we don’t know if this is true for young adults in Australia. This is what we will test today. Today we will be using the time_on_social, age, polit_informed, polit_campaign and polit_activism variables. The details of these variables are in the README.txt file, so you will already know what these variables stand for. However, let’s recap - these variables stand for the following: age – age in years time_on_social – average hours/day on social media (self-report diary) Political attitude subscales: polit_informed – how politically informed they feel (e.g., read news daily) polit_campaign – how much they engage in campaign-related discussion polit_activism – involvement in activism (e.g., protests, petitions) polit_informed, polit_campaign and polit_activism are subscales of a political attitudes questionnaire. Each subscale is scored out of 7, with higher scores indicating greater political attitude in that domain. 5.2.1 Activity - Get the data We will need to calculate our own political attitudes score, but first we need to load the data. Load the PSYC2001_social-media-data-cleaned.csv dataset into a data frame called social_media. HINT: If you are stuck, you can copy and paste this code from a previous week. Run the code and check that the data is loaded correctly using your preferred method from Section 2.6.1. 5.3 Building a composite score that measures political attitude Let’s now get an idea of our much political attitude these participants have. We have scores from 3 subscales of a political-attitudes questionnaire. We need to combine them into a single ‘political attitude’ score for each participant. Each subscale score carries a different weighting towards the total political attitude score. Weighting refers to the amount (proportion) that each subscale contributes to the single overall political attitude score, e.g. if a subscale has a weighting of 0.25, then it contributes one quarter (25%) of the total score. Pause for thought: Why would each subscale carry a different weighting? Think about what was discussed in the lectures about composite scores. 5.3.1 Activity - Get some political attitude The formula for calculating political attitude is: \\[\\mathrm{PoliticalAttitude} = 0.25 \\times \\mathrm{polit\\_informed} + 0.35 \\times \\mathrm{polit\\_campaign} + 0.4 \\times \\mathrm{polit\\_activism}\\] Now you have this formula, let’s build the code that creates the political attitude scores step by step, and test that its working properly. The first operation we need to do is to multiply each observation in the polit_informed column by 0.25. Examine the data frame using the View() function (or using another method if you prefer). Take the first 3 observations from polit_informed and multiply each of them by 0.25 (in the console, on a calculator, or in your head). What are the 3 results that you get? Write them as a comment in your script. Now we know what answers we should get, we can write some code to multiply these observations by 0.25, and check the code works. Remember, if we want to create a new variable in a data frame, then mutate() is a very handy function to use. Here is the code from Section 4.3.1 that gives a good example on how to use the mutate() function. This code is also in your script for this computing lab. Amend the below code in your script, so that you save the data frame to an object called social_media_test social_media_likes &lt;- social_media %&gt;% mutate(likes =(bad_mood_likes + good_mood_likes)/2 ) # creates a new variable called likes which is the average of bad_mood_likes and good_mood_likes Now amend the code so that you make a new column called test (instead of likes) that takes the column polit_informed and multiplies each entry by 0.25. Hint: You multiply in R using the * symbol. Run your new code and then run View(social_media_test) to check the results. Compare the first 3 values of the variable test to the results you got from your manual calculation. The next thing we need to do is multiply polit_campaign by 0.35, and add it to the previous result. Using the console (or your mind brain), calculate yourself what values you should get if you take the first few values of polit_campaign and multiply each of them by 0.35. Then add these new values to those you got when multiplying polit_informed values by 0.25. Write these new results down as a comment in your script. Now lets amend our code so that it can calculate these new values. Using the code you just wrote, add + 0.35 * polit_campaign to inside your mutate() function call. Run your new code. Check the contents of the test variable to make sure the first few numbers match your own calculations. Nice work! We tested each bit of the code and we now know that we are on the right path to calculating the composite scores for political attitude. We created the data frame social_media_test as a kind of scratch pad for testing. But we don’t need the results of those tests to complete our analysis. So let`s remove it from our environment, in a bid to keep the environment tidy. Run the following line of code to remove the social_media_test data frame from your environment rm(social_media_test) #removes the specified object from the environment Now lets properly create the political attitude scores, and save the resulting data frame to an object that has a useful name for our analysis. Complete the following line of code in your script social_media_attitude &lt;- social_media %&gt;% mutate(polit_attitude=...) Check the contents of the new data frame using your favourite method to check the new variable has been added See Section 2.6.1 if you want some inspiration on methods. ## X id age time_on_social urban good_mood_likes bad_mood_likes followers ## 1 1 S1 15.2 3.06 1 22.8 46.5 173.3 ## 2 2 S2 16.0 2.18 1 46.0 48.3 144.3 ## 3 3 S3 16.8 1.92 1 50.8 46.1 76.5 ## 4 4 S4 15.6 2.61 1 29.9 29.2 171.7 ## 5 5 S5 17.1 3.24 1 37.1 52.4 109.5 ## 6 6 S6 15.7 2.44 1 26.9 20.2 157.5 ## polit_informed polit_campaign polit_activism polit_attitude ## 1 2.3 3.2 3.6 3.135 ## 2 1.6 2.2 2.6 2.210 ## 3 1.9 2.7 3.0 2.620 ## 4 1.6 2.3 2.6 2.245 ## 5 2.0 2.9 3.3 2.835 ## 6 2.4 3.4 3.9 3.350 5.3.2 Activity - save the results of your hard work It is going to be useful for us to save the data frame that contains this new variable, as we will be using it again in the next computing tutorial. So let’s save the data frame as a ‘.csv’ file. Our social_media_attitude data frame has a lot of variables that we don’t need for this analysis, or the analysis we will run next week. So let’s tidy up the data a bit. We’ll reduce our data frame so that it only contains the columns: id, time_on_social, polit_attitude, age, and urban. For that, we can use the select() function, as we did in Section 4.3.1. Here is the relevant code from that section again: social_media_likes &lt;- social_media %&gt;% mutate(likes =(bad_mood_likes + good_mood_likes)/2 ) %&gt;% # creates a new variable called likes which is the average of bad_mood_likes and good_mood_likes select(id, urban, likes, followers) #selects only the specified columns from the data frame The key piece of code we need to add to our code and adapt so that we select only the relevant variables we want to keep (id, time_on_social,polit_attitude,age, urban) is the following: %&gt;% select(id, urban, likes, followers) Copy and paste the key piece of code to the relevant place in your script, and run your code to get your refined data frame. Check the resulting data frame looks as you would expect using your preferred method. You should get a data frame whose contents look like the below: ## id time_on_social polit_attitude age urban ## 1 S1 3.06 3.135 15.2 1 ## 2 S2 2.18 2.210 16.0 1 ## 3 S3 1.92 2.620 16.8 1 ## 4 S4 2.61 2.245 15.6 1 ## 5 S5 3.24 2.835 17.1 1 ## 6 S6 2.44 3.350 15.7 1 Now we are ready to save our refined data frame to a ‘.csv’ file for future use. If you remember, we did something very similar in Section 2.10.1, using the following code, which you will also find in your script. write.csv(social_media_NA, here(&quot;Output&quot;,&quot;PSYC2001_social-media-data-cleaned.csv&quot;)) #creates a csv file from the dataframe social_media_NA Amend the code above to save the social_media_attitude data frame to a file called “PSYC2001_social-media-attitude.csv” in the “Data” folder. Run your code and check the Data folder now contains the new file. Note that we previously saved the ‘.csv’ file to the “Output” folder, and this time we are saving to the “Data” folder. It can be a good idea to be careful about what you save to the Data folder when you first start out coding, because you want to make very, very sure you don’t overwrite the original data. But now we have grown and learned and we feel more confident saving something straight to the “Data” folder without incurring disaster. 5.4 How would political attitudes relate to social media use? Now that we have a single composite score that reflects an individual’s political attitude, take a moment to think about how political attitudes may relate to time spent on social media. 5.4.1 Activity - Noting down your hypotheses It’s time to define our hypothesis. Will young adults who spend a lot of time on social media have stronger political attitudes? Or the opposite? Or neither? What are your thoughts? What is a reasonable null and alternate hypothesis? Write it down as a comment in your script. 5.5 Looking for lines Now let`s visualise our data! We’re going to explore the relationships between our variables to see if it is appropriate to run a Pearson correlation analysis. Remember, we use a Pearson correlation analysis when we think a straight line is a reasonable approximation of the relationship between our variables. Figure 5.1: MFW people don’t visualise their data 5.5.1 Activity - Scatterplots to visualise relationships (in the hunt for straight-ish lines) Let’s use ggplot to make scatterplots to show the relationships between time_on_social, polit_attitude, and age. You can see a little bit about scatterplots and how to implement them in R here. We have given you the code for the first scatterplot. Run the code to make this scatterplot of the relationship between polit_attitude and time_on_social. You should see a scatterplot that looks like the one below. social_media_attitude %&gt;% ggplot(aes(x = polit_attitude, y = time_on_social)) + # set up the canvas geom_point(colour = &quot;orange&quot;) + # make a scatterplot labs(x = &quot;Political Attitude&quot;, y = &quot;Time on Social&quot;) + # define labels theme_classic() # make pretty ## Warning: Removed 2 rows containing missing values or values outside the scale range ## (`geom_point()`). Note: we get a warning message. The message is telling us that there are 2 rows containing missing values. That’s ok. We know this, because we turned the -999s in the time_on_social variable into NAs in Section 2.9.1. We also learned then that there were 2 participants with missing data for time_on_social. So we can safely ignore this warning message. It is always a good idea to check, when getting a warning message, that you understand what is causing it. What do you think? Would you say there is a linear relationship between political attitude and time spent on social media? Imagine drawing a straight line through the cloud of dots. Is there a straight line you can draw that the data points are consistently clustered around? (i.e. there is roughly the same number of data points above and below the line at every point along the x-axis). Now, using the above code as a guide, create two more scatterplots: one of the relationship between polit_attitude and age, and another of time_on_social and age. You know you will have completed your mission when you have produced plots that look something like the following: Remember to check you have adjusted the axis labels! Question: Take a look at the relationships depicted in these latter two scatterplots? Hint: one of them shows a non-linear relationship. Which one is it? A non-linear relationship is present when the relationship between two variables does not appears to roughly follow a straight line. One example is when there is a curve shape. Remember that we do not want to conduct a Pearson’s correlation analysis when the relationship is non-linear. 5.6 Conducting correlations First, let’s calculate the correlation coefficient between the variables that appear to share a linear relationship. Let’s start with time_on_social and polit_attitude. If you remember from your lecture, the correlation coefficient is a number that tells us the strength and direction of the linear relationship between two variables. 5.6.1 Calculate the correlation coefficient Instead of calculating the correlation coefficient by hand, we can use a handy function called cor to do it for us. Even more handy, cor can be used in a pipe. So calculation the correlation coefficient between time_on_social and polit_attitude is as simple as: social_media_attitude %&gt;% summarise(r = cor(time_on_social, polit_attitude, use = &quot;complete.obs&quot;)) #use = &quot;complete.obs&quot; removes all NA values from the correlation. ## r ## 1 0.5125282 This tells us that there is a positive linear relationship between time spent on social media and political attitude, with a correlation coefficient of approximately 0.51. This suggests that as time spent on social media increases, political attitude also tends to increase. Given that correlation co-efficients range from -1 to +1, a value of 0.51 indicates a moderate positive correlation. 5.7 Testing the statistical significance of the correlation coefficient However, we don’t stop once we have calculated the correlation coefficient (\\(r\\)). We need to know if this value of \\(r\\) is larger than we would expect to get by chance. Enter our hypothesis test for \\(\\mathrm{H_o}\\)! 5.7.1 Activity - correlation using the formula method We can use the cor.test() function to test \\(\\mathrm{H_o}\\). We can use the cor.test() function either using the formula method, or using base R. We’ll do both, so that your coding journey is imbued with liberty. The cor.test() function can take in a formula where the right hand side specifies the two numeric variables to be correlated with each other. Note that this is a little different to when we used the formula method in Section 4.6.1, where our formula followed the syntax of DV ~ group. Specifically, we had a dependent variable on the left hand side, and a grouping variable on the right hand side. Thus, this formula says that we want to know how the DV changes as a function of group. With a correlation analysis, we instead have two observed variables (i.e. we did not manipulate anything but just observed time spent on social media and political attitudes as they occurred in the wild), and we want to know what the correlation value is, as a function of those two variables. So, this time, we have not bothered defining a DV on the left hand side. This is because we are saying we want to know the correlation as a function of our two numeric variables (time_on_social and polit_attitude). And because we are passing the formula into the correlation function, R knows what we are asking for, and so we don’t need to define a DV. Knowing when you do and do not need to put a DV on the left hand side of a formula is something that comes with practice. Run the following code in your script to run a correlation test on the relationship between time_on_social and polit_attitude. cor.test(formula = ~ time_on_social + polit_attitude, data = social_media_attitude, use = &quot;complete.obs&quot;) #formula contains both numeric variables on the right hand side. ## ## Pearson&#39;s product-moment correlation ## ## data: time_on_social and polit_attitude ## t = 4.4667, df = 56, p-value = 3.904e-05 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.2930241 0.6807091 ## sample estimates: ## cor ## 0.5125282 #use = &quot;complete.obs&quot; removes all NA values from the correlation. Question: Have a look at the output in the console. What is the relationship between time_on_social and attitude? Can you reject the null hypothesis? Was your hypothesis supported? What does the ‘df’ value tell you about the sample size? Why is it that number? Add your answers to the questions above as a comment to your code. 5.7.2 Activity - correlation using base R syntax The cor.test() function not only accepts formulas, but also base R syntax. In base R syntax, we specify the two variables to be correlated using the x= and y= arguments. To specify the x= and y= arguments, we will need to use the $ operator as we did in Section 3.7.1. Complete the following code in your script and run it to check you get the same output as when you ran a correlation test using the formula method. Use the code from Section 3.7.1 as a guide, if you need. cor.test(x = social_media_attitude$..., y = social_media_attitude$..., use = &quot;complete.obs&quot;) Which method should I use? It will largely come down to your preference. You might prefer to think about the correlation in terms of your x variable and your y variable, or the formula may make more sense to you. The great thing about coding is that you can use the solution that suits you best. 5.8 Writing up results and conclusions Now lets have a go at writing up the results of the correlation that we have conducted: Results: A pearson correlation was performed to evaluate the relationship between political attitude and time spent on social media. It was found that there was a strong positive correlation between political attitude and time spent on social media (\\(r\\)(56) = .51, \\(p\\) &lt; .01). 5.8.1 Activity - Correlation for the other pair of variables Now, run the correlation analysis for the other pair of variables that shared a linear relationship. I know we haven’t told you which one it is, so make a guess and ask your tutor if you are not sure. Copy and paste the code you wrote earlier for the correlation analysis into your script, from first using the cor() function, up to performing the statistical test of \\(\\mathrm{H_o}\\). Amend the code so that it runs a correlation analysis on the other pair of variables that shared a linear relationship. Run your code and interpret the results. 5.9 You are Free! Well done folks, you have survived another computing tutorial! Next lesson we will be conducting linear regression on this data, so well done you for saving your data set. Figure 5.2: This is you now 5.10 ⭐ Bonus exercises 5.10.1 Bonus Activity - A closer look at subscales We created a composite score for political attitude by combining 3 subscales. But what do we know about each of the subscales? Can you take the social_media_attitude data frame and plot the data for each of the 3 subscales: polit_informed, polit_campaign and polit_activism? There is always inspiration available at the R Graph Gallery. 5.10.2 Bonus Activity - Urban vs Rural Correlations Would you expect the correlation between political attitude and time spent on social media to be the same for urban and rural participants? We can answer that! Can you take the social_media_attitude use the group_by() and summarise() functions to build a set of pipes where you take the social_media_attitude data frame, and the result is separate correlation coefficient values (\\(r\\)) between the time_on_social and age variables for Urban and Rural participants? Check how you previously used the summarise() and group_by() functions for clues! 5.10.3 Bonus Activity - Correlation Matrix Sometimes it`s helpful to see all the correlation coefficients for all the variables that you are interested in at once. Can you use the select() and cor() function to create a pipe that takes the social_media_attitude data frame, and that produces a correlation matrix that shows the correlations between all the variables you are interested in? Important info make sure to only pick continuous variables, if not you will likely wind up down a garden path of nefarious errors. This might be a tricky one. You could ask your favourite LLM for help, and then test and comment on the answer. Do the correlation values match what you would expect? What else do you notice about the correlation matrix? "],["linear-regression.html", "Chapter 6 Making predicitions using Linear Regression 6.1 Checking installation and loading packages 6.2 Investigating linear regressions 6.3 Let`s get the data into shape for some viz 6.4 Linear regression in R 6.5 Writing up results and conclusions 6.6 Plotting our results 6.7 You are Free! 6.8 ⭐ Bonus exercises", " Chapter 6 Making predicitions using Linear Regression This week, we are going to continue to explore relationships between continuous variables, using simple linear regression. 6.1 Checking installation and loading packages As usual we first always load in our required packages here and tidyverse. 6.1.1 Activity - load the packages! Find the code from a previous week that loads the packages here and tidyverse. Copy it to your script and run the code. 6.2 Investigating linear regressions Last week we looked at the correlation between social media use, age and political attitude. We created a composite score to measure overall political attitude, and we saved a data set that contains the new score. Today we will be using a simple linear regression to determine the extent to which political attitude can predict the amount of time spent on social media. Here is a recap of the variables: id - participant ID age – age in years time_on_social – average hours/day on social media (self-report diary) polit_attitude - political attitude score composed of a weighted sum of the subsales informed, campaign and activism 6.2.1 Activity - Formulate your prediction about predictions Before we begin coding, let’s think about our research question. We learned last week that there is a statistically significant correlation between political attitude and time spent on social media. This tells us that we should be able to somewhat predict time spent on social media from political attitude. What do you think the nature of this predictive relationship will be? Will political attitude positively or negatively predict time spent on social media? How much of the variance in time spent on social media would you expect to be predicted by political attitude? Write in the comments of your code whether you expect a positive or negative relationship, and if you are feeling statistically empowered, what proportion of variance of time spent on social media you would expect to be accounted for by political attitude. Question: What is a reasonable null and alternate hypothesis? Add this to the comments in your code. 6.2.2 Activity - loading the data set First, we need to load our data set for today. This week we will be using the PSYC2001_social-media-attitude.csv data set which includes the attitude variable we created last week. Copy the code you have used to load the data in previous weeks, and paste it into your script. Amend the code so that you are loading the correct data set for today. Make sure you assign it to an object named social_media_attitude. Check the data set is as you think by running the line of code head(social_media_attitude). 6.3 Let`s get the data into shape for some viz Before fitting the regression, let’s look at the distributions of our key variables using density plots, similar to as we did in Section 3.4.1. Remember, visualising first helps us spot outliers and get a feel for the data—remember: garbage in, garbage out. We’ll reuse the same tidy workflow you used back then: we will select() the columns we need, pivot_longer() into longform, then make density plots. 6.3.1 Activity - select and pivot to longform The goal: move the values from time_on_social and polit_attitude into one column and keep a second column that labels which variable the value came from. Run the following code in your script to see what it does to the data frame. # Choose the columns, then pivot to longform social_media_attitude %&gt;% select(id, time_on_social, polit_attitude) %&gt;% # choose columns pivot_longer( # go from wide -&gt; long cols = c(time_on_social, polit_attitude), # the two key measures names_to = &quot;measure&quot;, # which measure is this value from? values_to = &quot;value&quot; # the numeric value itself ) Now, we want to save this new longform data frame so that we can use it to make us some density plots Assign the new data frame to an object called sma_long. Check the result by running the line of code that says head(sma_long) Here, ‘sma’ stands for ‘social_media_attitude’. We shortened the name in a meaningful way to make it easier to type, and to make it easier to follow the code. (The shortening is meaningful because we can still tell what the object is about when it is called sma_!) You should see the following: ## # A tibble: 6 × 3 ## id measure value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 S1 time_on_social 3.06 ## 2 S1 polit_attitude 3.14 ## 3 S2 time_on_social 2.18 ## 4 S2 polit_attitude 2.21 ## 5 S3 time_on_social 1.92 ## 6 S3 polit_attitude 2.62 Helpful fact: In the longform data frame, each participant now has two rows (one for time_on_social, one for polit_attitude). The eagle-eyed among you will have noticed that R is calling your new data frame a ‘tibble’. Tibbles are a special type of data frame that are part of the tidyverse package. They have some nice features that make them easier to work with than regular data frames, such as better printing and subsetting. Don’t worry too much about tibbles for now, but just know that they are bascially a data frame, and that everything you know about data frames applies to tibbles. 6.3.2 Activity - examining distributions with density plots Now we want to plot both the polit_informed and time_on_social variables as density plots. As we have already written code to make a density plot in Section 3.4.2, let’s take that code and adapt it for our current purposes. That code is below (which will also be found in your script). Amend the below code so that we can use it with the object sma_long instead of social_media_likes. Change the code so that x = takes the value column instead. Remove the group and fill arguments as we don’t have groups in our data right now. social_media_likes %&gt;% ggplot(aes(x = likes, group = mood, fill = mood)) + # set canvas aesthetics geom_density() # use the data to draw a density plot When you are done, you should have something that looks like this: Now, this is no good, as we have a density plot of the variable value which contains the rows for both the polit_attitude and age values. And we want separate density plots, one for polit_attitude and one for age. We need to tell ggplot to separate them! Luckily, there is a very handy function called facet_wrap() that will do this for you. facet_wrap is your friend. To use the facet_wrap() function, we use the formula method, but we only need to put something on the right hand side, and that something is the name of the variable that we want to split up the plots by. Add the piece of code + facet_wrap(~measure) to your existing code you wrote to plot the density of the data. Run the code. You will know you have succeeded when you behold the same plot as below. ## Warning: Removed 2 rows containing non-finite outside the scale range ## (`stat_density()`). Excellent. We have plots. Question: Would you say the variables are normally distributed? What values for polit_attitude and time_on_social are more likely and which are less likely? Do these values seem sensible, given the scales for political attitude (0-7), and your experiences with social media? Write your answer to the questions above as a comment in your code 6.4 Linear regression in R The difference between a regression and correlation is that instead of looking for a statistical relationship between \\(x\\) and \\(y\\), we are asking how well we can predict \\(y\\) on the basis of \\(x\\). \\(x\\) is our predictor variable, and \\(y\\) is what is being predicted, which we refer to as our criterion variable or our dependent variable (DV). Running a regression is like asking “if we were stuck on a desert island with only \\(x\\), how much would we know about what \\(y\\) is doing?” 6.4.1 Activity - fitting a linear regression model Let’s run a simple linear regression to determine how well political attitude predicts the amount of time spent on social media. To do this we can we can fit a linear regression model using the lm() function. In the console, use the ? syntax to look up the help documentation for the lm() function. Write in your comments the first two arguments required for this function, and your guess as to what each argument is asking for. Hint: scrolling down to look at the examples may help you work it out. You need to complete the unfinished comment in your code that looks like this: # 1. the first two arguments for lm() are... The first argument is formula, which is where we specify the dependent and independent variables in our regression. We have already used formulas in Sections 4.6.1 and 5.7.1. In fact, the formula we need looks very similar to the one in Section 4.6.1, except that we now have a different DV (or criterion), and a continuous predictor rather than a categorical IV. Write in the comments of your code the formula that you will need to predict the time spent on social media from political attitude scores. Complete the unfinished comment in your code that looks like this: # 2. the formula I need to use is ... The second argument is data, which, just like in Section 4.6.1, is where we specify the data frame that contains our variables. Complete the following line of code in your script, so that you are fitting a linear regression model that predicts time_on_social from polit_attitude, using the data frame social_media_attitude. The code is writted so that it assigns the output of the model to an object named mod (which is short for model). mod &lt;-lm(formula = ... ~ ..., data = ... ) The first thing we should do is look at what the lm() function passed to our object model. Run the following line of code in your script to see what was saved to the object mod by the lm() function. You will see what has been saved to the object mod. mod ## ## Call: ## lm(formula = time_on_social ~ polit_attitude, data = social_media_attitude) ## ## Coefficients: ## (Intercept) polit_attitude ## 1.2853 0.4837 You can see that there is a documentation of how lm() was used (After ‘Call:’). It can be good to check this so that you can be sure you ran the correct model. After that, there are two coefficients output. The first is the Intercept (which we have referred to as \\(a\\) in the lectures). The second is the slope of the regression line (which we referred to as \\(\\mathrm{Beta}\\) in the lectures). Use the two coefficients to write a regression equation that predicts time spent on social media on the basis of political attitude in the comments in your code. Is the regression coefficient positive or negative? Is this what you expected? 6.4.2 Activity - how good is our model? We are able to make predictions about time spent on social media using the regression equation we have just created. But we are yet to find out how good our model is at making these predictions. For this, we need to extract the \\(R^2\\) value, and determine whether our model predicts variance to an extent that is greater than what is expected by chance. Luckily, R has a helpful summary() function that will take a linear regression as its input, and it will output us lots of useful information about the regression model. Run the following line of code in your script and examine the output of the summary() function, which appears in the console. You should see a table that looks like this: summary(mod) ## ## Call: ## lm(formula = time_on_social ~ polit_attitude, data = social_media_attitude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.43336 -0.46974 -0.02158 0.51194 1.36644 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.2853 0.2932 4.384 5.19e-05 *** ## polit_attitude 0.4837 0.1083 4.467 3.90e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.643 on 56 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.2627, Adjusted R-squared: 0.2495 ## F-statistic: 19.95 on 1 and 56 DF, p-value: 3.904e-05 Identify the \\(R^2\\) value in the output of the summary() function, and write it in the comments of your code. Further, identify the F statistic, degrees of freedom and p-value associated with the overall model fit, and write these in the comments of your code too. Finish with a brief comment about whether the result is as you expected or not. Note: R refers to the \\(R^2\\) value as ‘Multiple R-squared’ which is not the technically correct name when there is only one predictor in the model (the technically correct name is ‘R-squared’). R is just covering its bases here, assuming that one day you will have multiple predictors (PSYC3371, anyone?) and thus using the term ‘Multiple R-squared’ covers all the cases there might be, including the particular case of R-squared. Info: There are additional statistics that are output from the lm() model. We can leave them for future us at this point, but if you always need to unwrap your gifts immediately then you can read this blog here to find out more. 6.5 Writing up results and conclusions This is how you would write up the results of the linear regression we have just conducted. Results: A simple linear regression was performed to evaluate the extent that political attitude predicted the amount of time spent on social media. It was found that political attitude predicted the amount of time spent on social media to an extent that was statistically significant (\\(F\\)(1,56) = 19.95, \\(p\\) &lt; .01, \\(\\mathrm{R}^2\\) = 0.26). 6.6 Plotting our results To show how well a simple linear regression captures the relationship between two variables you should include a straight line of ‘best fit’ on a scatterplot. This is incredibly useful at visually informing the reader of the linear predictive relationship between variables. (Un)Surprisingly, most people (including researchers) are quite bad at guessing the strength of linear relationships using scatterplots alone ! 6.6.1 Activity - Adding a line of best fit to a scatterplot The line of best fit can easily be added to a ggplot graph by using the geom_smooth function. Below, we have pasted the code that plotted the scatterplot of political attitude against time spent on social media from last week, in Section 5.5.1. We have added one extra line of code that adds the line of best fit to the plot. Can you spot it? social_media_attitude %&gt;% ggplot(aes(x = polit_attitude, y = time_on_social)) + # set up the canvas geom_point(colour = &quot;orange&quot;) + # make a scatterplot geom_smooth(method = &quot;lm&quot;, se = TRUE) + # add line of best fit and ... labs(x = &quot;Political Attitude&quot;, y = &quot;Time on Social&quot;) + # define axis labels theme_classic() # make pretty Use the ? operator to look up the help documentation for geom_smooth(). Find out what the se = argument stands for, and finish the comment in your code with a description of what it does. Run the code in your script to get the below plot. Is the line where you thought it would be? 6.6.2 Activity - level up You will also see in the help documentation that there is an argument called level =. Add it to the call for geom_smooth(), setting it to level = 0.95. Remember to make sure there is a comma between the last argument and this one. Run the code again. What has changed about the plot? Now change the level = argument to some other number (other than 0.95). Run the code again. What has changed now? Add to the comment in your code what the level() argument does. 6.7 You are Free! Well done folks, you have survived another computing tutorial. If you are unsure about any of the contents then your friendly local tutor is here to help! Figure 6.1: Done. We are all friendly neighbourhood statisticians now 6.8 ⭐ Bonus exercises Are you ready for the challenge of the very last bonus exercises? 6.8.1 Bonus Activity - Tidying up the Scatterplot I don’t know about you, but the default theme for the line of best fit smites my eyes. Can you alter the colours of the line of best fit and the shaded area so that it fits better with the colour scheme of the rest of the plot? Can you find some examples of how to do this on stackoverflow.com, or can an LLM help you find the answer? 6.8.2 Bonus Activity - Visualising the line of best fit for Urban and Rural participants on separate plots It could be useful to know whether the predictive relationship between political attitude and time on social is comparable for Urban and Rural participants. Can you amend your code for the scatterplot with the line of best fit, so that the plots for Urban and Rural participants are shown on separate panels? Hint: You have already used the function that will do this for you earlier in this section. For an extra level up, can you change the aesthetics so that the points are coloured by urban (Urban or Rural), and the line of best fit is also coloured by urban? 6.8.3 Final Bonus Activity - Can you fix the bug to make this code work? This one is fiendish! Below is a block of code that is supposed to create a scatterplot of the Observed vs Predicted values from our linear regression model. The code has been commented so that you can understand what it is trying to do. However, there is a bug in the code that is stopping it from working. # Activity – Observed vs predicted plot social_media_attitude %&gt;% # take the data frame mutate(predicted = predict(mod)) %&gt;% # create a new column that contains the predicted value for each participant ggplot(aes(x = predicted, y = time_on_social)) + # set up the canvas geom_point(alpha = 0.6) + # make a scatterplot, and make the dots kinda seethrough geom_abline(slope = 1, intercept = 0, colour = &quot;red&quot;, linetype = &quot;dashed&quot;) + # add a 45 degree reference line. If predictions were perfect, all points would fall on this line labs(x = &quot;Predicted&quot;, y = &quot;Observed&quot;) + # define labels theme_classic() # make pretty If you run this code you should get the following error message. The important part for working out this bug is highlighted for you. Figure 6.2: The fiendish bug lies therein The other clue I will give you is that you can check how many rows are in a data frame by using the nrow() function. For example, if you run the line of code nrow(social_media_attitude), you will see how many rows are in that data frame. (You can also look at social_media_attitude in the Environment pane in RStudio to see how many rows it has). The very last clue is in Section 2.9.1. OK, the last last clue is that there is a tidyverse function that will help you. Good luck! "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
